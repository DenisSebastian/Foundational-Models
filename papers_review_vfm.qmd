---
title: "Papers Review: VFM"
subtitle: "Avances recientes en modelos de fundamentales de Visión en la Percepción Remota"
---

## Introducción

### Contexto general

El Remote Sensing (teledetección) ha emergido como una herramienta crucial para la observación y análisis del planeta Tierra, permitiendo monitorear fenómenos ambientales y territoriales con un nivel de detalle sin precedentes. Tecnologías como las imágenes satelitales y los sistemas aéreos han facilitado la recopilación continua de datos espaciales de alta resolución, lo que es fundamental para aplicaciones como la detección de cambios ambientales, la gestión de desastres naturales, el monitoreo de recursos y la planificación territorial.

En este contexto, los modelos fundacionales (Foundation Models, FMs) representan una innovación disruptiva en el campo de la inteligencia artificial aplicada a la percepción remota. Estos modelos preentrenados sobre grandes volúmenes de datos no etiquetados son capaces de aprender representaciones generales que pueden ajustarse fácilmente a tareas específicas utilizando cantidades relativamente pequeñas de datos anotados. Su flexibilidad ha permitido abordar problemas complejos en teledetección, como la segmentación semántica, la detección de objetos y la clasificación de escenas, optimizando así el análisis de datos multiespectrales y temporales.

Los avances recientes en Visual Foundation Models (VFMs), basados en arquitecturas avanzadas como los Vision Transformers (ViT) y enfoques autosupervisados, han potenciado su aplicabilidad a datos de alta resolución. Este progreso se alinea con la creciente capacidad de procesamiento computacional y la disponibilidad masiva de imágenes satelitales de alta calidad, las cuales aumentan continuamente en resolución espacial y temporal. Estas características posicionan a los VFMs como una herramienta prometedora para enfrentar desafíos territoriales y ambientales, particularmente en regiones como Chile y América Latina, donde los datos geoespaciales son abundantes pero subutilizados.

### Objetivos de la revisión

El propósito de esta revisión bibliográfica es:

1.  Explorar el estado del arte en el desarrollo y aplicaciones de los modelos fundacionales en percepción remota, con un enfoque en los Visual Foundation Models (VFMs).
2.  Analizar su potencial para abordar problemas territoriales específicos en contextos geográficos de Chile y América Latina.
3.  Identificar oportunidades y desafíos para la implementación de estos modelos, considerando tanto la creciente disponibilidad de datos geoespaciales como las capacidades computacionales actuales.
4.  Proponer una base conceptual que sirva como fundamento para evaluar y adaptar VFMs en aplicaciones concretas, maximizando su impacto en problemas como planificación territorial, monitoreo ambiental y manejo de desastres.

## Metodología

### Criterios de Inclusión

Para garantizar que los estudios seleccionados sean relevantes y alineados con los objetivos de esta revisión, se definirán los siguientes criterios de inclusión:

1.  **Relevancia temática**

-   Estudios centrados en modelos fundacionales de visión (VFMs) aplicados a teledetección, específicamente en tareas como detección de cambios, clasificación de uso de suelo, segmetación semántica.
-   Investigaciones relacionadas con el uso de datos geoespaciales, imágenes satelitales o aéreas en problemas ambientales y sociales.

2.  **Reciente publicación**

-   Publicaciones desde 2020 en adelante, considerando la novedad del campo y su evolución rápida.
-   Publicaciones en revistas indexadas o conferencias de alto impacto en áreas de inteligencia artificial, percepción remota o teledetección.

3.  **Relevancia geográfica y temática**

-   Estudios que incluyan aplicaciones en problemas reales relacionados con el medio ambiente y las personas, como:
-   Cambio climático, gestión de recursos naturales, agricultura sostenible, planificación urbana, monitoreo de desastres o biodiversidad.
-   Preferencia por estudios que incluyan contextos aplicables a Chile o América Latina, aunque no se excluirán estudios con enfoques globales relevantes.

4.  **Disponibilidad de resultados, métricas y códigos**

-   Estudios que incluyan evaluaciones cuantitativas de desempeño, como métricas de clasificación, segmentación o detección (e.g., IoU, precisión, F1-score).
-   Descripción detallada de los datasets utilizados y la metodología aplicada, eventualmente códigos.

5.  **Enfoque en arquitectura y métodos de aprendizaje**

-   Modelos basados en arquitecturas avanzadas, como Vision Transformers (ViT) y enfoques autosupervisados (e.g., MAE, contrastive learning).
-   Innovaciones metodológicas para optimizar el uso de datos no etiquetados o resolver problemas multiespectrales y multitemporales.

## Marco Conceptual

A continuación, se organiza y define cada concepto clave relevante al tema de la revisión bibliográfica sobre modelos fundacionales de visión aplicados a teledetección. Estos conceptos proporcionan el fundamento teórico necesario para comprender el desarrollo y aplicación de los Visual Foundation Models (VFMs) en problemas territoriales y ambientales.

![](images/VFMs_esquema.png)

Son modelos de aprendizaje profundo preentrenados sobre grandes volúmenes de datos no etiquetados, diseñados para tareas de visión por computadora. Su arquitectura avanzada, basada en enfoques como Vision Transformers (ViT), los hace altamente adaptables a una variedad de aplicaciones en teledetección, desde segmentación semántica hasta detección de cambios.

### Autoencoders Enmascarados (MAE)

Los MAE son modelos de aprendizaje profundo diseñados para reconstruir datos de entrada a partir de una versión parcialmente enmascarada.

- Aplicación en VFMs: En el contexto de la teledetección, los MAE son clave para procesar imágenes satelitales, permitiendo al modelo aprender representaciones robustas incluso con datos incompletos o ruidosos.

### Aprendizaje Contrastivo

Es una técnica de aprendizaje autosupervisado que entrena al modelo para distinguir entre pares de datos similares (positivos) y disímiles (negativos).

- Relevancia en VFMs: Es utilizado en modelos como CROMA para aprender representaciones invariantes a sensores múltiples (e.g., radar y ópticos).

### Datos Multimodales y Alineación Espacial

- Datos Multimodales: Provienen de sensores diferentes (e.g., Sentinel-1 para radar y Sentinel-2 para óptico), enriqueciendo las representaciones aprendidas por los VFMs.
- Alineación Espacial: Garantiza que los datos de distintos sensores correspondan a la misma ubicación geográfica, asegurando consistencia en las representaciones aprendidas.


### Codificación de Posición Relativa (RPE)

Técnica utilizada para incorporar información posicional en modelos basados en transformers.
- Impacto en VFMs: Mejora la capacidad de generalización de modelos como CROMA en imágenes más grandes.


	

## Revisión de los Estudios

Para organizar y analizar los artículos científicos se construyó una tabla que resume información importante para cumplir con los objetivo del presente capítulo, los criterios y contenidos estatal corresponde a los siguientes:

- Referencia: Mencionar los autores y año, incluyendo una referencia breve para seguimiento.
- Objetivo del Estudio: Resumir el problema y la tarea específica abordada.
- Arquitectura: Detallar el modelo utilizado (e.g., ViT, Swin Transformer, MAE).
- Dataset: Especificar los datasets usados, incluyendo resolución y tipo (e.g., SAR, multiespectral).
- Aplicación: Indicar cómo se aplica el modelo (e.g., monitoreo de inundaciones, detección de cultivos).
- Métricas: Proporcionar los valores clave de desempeño.
- Limitaciones: Identificar las barreras técnicas o de aplicación señaladas en el estudio.
- Relevancia para Chile/LatAm: Evaluar la aplicabilidad en contextos locales según problemas ambientales o sociales.
	
### Tabla Resumen General
| **Referencia**                                     | **Objetivo del Estudio**                                                                                                                                                 | **Arquitectura**                                                                 | **Dataset**                                                                                                                                                      | **Aplicación**                                                                                       | **Métricas**                                                                                                   | **Limitaciones**                                                                                          | **Relevancia para Chile/LatAm**                                                                                                    |
|---------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| Fuller et al. (2023)                              | Desarrollar un framework para aprender representaciones unimodales y multimodales para la Observación de la Tierra, aprovechando objetivos autosupervisados contrastivos y de reconstrucción. | CROMA (autoencoder contrastivo radar-óptico enmascarado), basado en Vision Transformer (ViT), con codificación de posición 2D-ALiBi y X-ALiBi. | BigEarthNet, fMoW-Sentinel, EuroSAT, Canadian Cropland, DFC2020, DW-Expert, MARIDA. Usando imágenes Sentinel-2 (óptico) y Sentinel-1 (radar) a diferentes resoluciones. | Clasificación de imágenes satelitales y segmentación semántica.    | Supera el estado del arte (SatMAE) en un promedio de 5.4% y 6.4% en mIoU para backbones ViT-B y ViT-L, respectivamente.                              | Requiere datos multimodales alineados espacialmente. Alto costo computacional para entrenar modelos grandes. | Alta: Chile posee una amplia cobertura de imágenes Sentinel, lo que permitiría aplicar CROMA para diversas aplicaciones como la clasificación de uso y cobertura del suelo, monitoreo de recursos hídricos, agricultura de precisión, entre otras. |
| Wang et al. (2022)                                | Proponer una red neuronal (ASN) para la detección de cambios bi-temporales, integrando pares de características asimétricas y refinamiento de características.           | Adaptive Symmetric Network (ASN) con codificador-decodificador, utilizando bloques residuales, Xception o Squeeze-and-Excitation.                     | SECOND dataset (multi-clase, hasta 36 tipos de cambio). No se especifica la resolución ni los sensores.                                      | Detección de cambios en imágenes satelitales para análisis de urbanización, desastres naturales, etc. | ASN-ATL supera HRSCD.str4 en 1.9 en mIoU y 2.2 en SeK con bloques Xception.                                                         | No se especifica la resolución de las imágenes ni los sensores utilizados.                              | Media: La detección de cambios es relevante para Chile, por ejemplo, para el monitoreo de la deforestación o la expansión urbana. La aplicabilidad de ASN dependerá de la disponibilidad de datasets con características similares a SECOND. |
| Chen et al. (2023)                                | Aprovechar el conocimiento general de un modelo fundacional (SAM) para construir una red de detección de cambios eficiente (TTP).                                        | Time-Traveling Prompt (TTP), basado en el modelo fundacional Segment Anything Model (SAM), con ajuste fino de rango bajo, una puerta de activación de viaje en el tiempo y un decodificador multinivel. | LEVIR-CD dataset. No se especifica la resolución ni los sensores.                                                                                                 | Detección de cambios en imágenes satelitales para el análisis del desarrollo urbano, monitoreo de desastres naturales, etc. | TTP logra un F1 de 92.1 e IoU de 85.6, superando a métodos como WNet y CSTSUNet.                                                   | Dependencia del modelo fundacional SAM. No se especifica la resolución ni los sensores de las imágenes.         | Media: El uso de modelos fundacionales como SAM para la detección de cambios en Chile es prometedor. Se requiere investigar la adaptabilidad de TTP a las condiciones locales. |
| Muszynski et al. (2024)                           | Evaluar la capacidad de los modelos fundacionales geoespaciales para estimar la biomasa aérea a partir de imágenes satelitales con pocos parámetros ajustables.          | Dos modelos fundacionales geoespaciales (Global GFM y Local GFM) con encoder congelado y una capa de regresión.                                      | Imágenes HLS de la NASA de tres ecorregiones en Brasil. Resolución de 30 metros.                                                                                 | Estimación de biomasa aérea para el estudio del ciclo del carbono y la gestión forestal.             | Los GFMs con encoder congelado logran un rendimiento comparable al de una U-Net entrenada desde cero, con 13 veces menos parámetros.                                            | La generalización a otras regiones puede verse afectada por la variabilidad en las características del bosque.    | Media: La estimación de biomasa aérea es relevante para el monitoreo forestal en Chile. Se necesita evaluar la transferibilidad de los GFMs a los bosques nativos del país.                                        |
| Tseng et al. (2023)                               | Aborda la escasez de datos etiquetados en la teledetección mediante el desarrollo de Presto, un modelo ligero de Transformers preentrenado para series temporales.       | Transformers con codificador-decodificador, basado en Masked Autoencoding        | Diversos: CropHarvest, TreeSatAI, EuroSAT, Sen1Floods11, S2-Agri100; resolución y tipo de datos variables (SAR, multiespectral).             | Clasificación de cultivos, detección de especies de árboles, clasificación de cobertura del suelo, mapeo de inundaciones, monitoreo de cultivos. | Precisión variable según la tarea; en algunos casos supera a los modelos de última generación.                     | Limitado a datos de series temporales de píxeles. Requiere adaptación para imágenes de alta resolución.      | Alta: útil para monitoreo agrícola, gestión forestal, y evaluación de riesgos naturales en Chile y LatAm.                        |
| Hsu and Arundel (2023)                            | Evalúa el desempeño del modelo geoespacial Prithvi para el mapeo de inundaciones.                                                                                        | Transformers (ViT) preentrenado con Masked Autoencoder (MAE)                      | Sen1Floods11; imágenes Sentinel-1 y Sentinel-2, resolución de 10m.                                                                           | Mapeo de inundaciones                                                                                  | Transferibilidad en datos no vistos; menor precisión que U-Net y Segformer en datos conocidos.                          | Requiere 3 bandas adicionales a RGB y carece de arquitectura de extremo a extremo para análisis de alto nivel. | Alta: relevante para evaluación de daños, planificación de emergencias y diseño de sistemas de alerta temprana en Chile.          |
| Mall et al. (2023)                                | Sin información específica sobre el estudio en las fuentes.                                                                                                             | No especificada.                                                                  | No especificada.                                                                                                                                                 | No especificada.                                                                                        | No se proporcionan métricas clave.                                                                           | No especificada.                                                                                           | Sin información suficiente para evaluar relevancia.                                                                               |
| Li et al. (2023)                                  | Investiga métodos SSL para reconocimiento automático de objetivos (ATR) en imágenes SAR; propone SAR-JEPA para predicción de gradientes multiescala.                    | Basado en Masked Image Modeling (MIM) y Joint-Embedding Predictive Architecture (JEPA). | MSTAR, FUSAR-Ship, SAR-ACD; imágenes SAR.                                                                                                     | Reconocimiento de objetivos en imágenes SAR (vehículos, barcos, aviones).                               | Supera métodos SSL en clasificación con pocos disparos.                                                   | Requiere grandes datasets SAR para preentrenamiento.                                                        | Moderada: útil para monitoreo de deforestación, pesca ilegal, y evaluación de desastres en Chile y LatAm.                      |
| Reed et al. (2023)                                | Explora el aprendizaje de representaciones geoespaciales multiescala utilizando Scale-MAE, un MAE sensible a la escala.                                                  | MAE con codificador ViT y decodificador piramidal Laplaciano                      | Functional Map of the World (FMoW), RESISC-45, entre otros; imágenes RGB y multiespectrales de diferentes resoluciones.                       | Clasificación de uso del suelo, segmentación semántica (detección de edificios).                           | Muestra robustez a la escala y supera a SatMAE y ConvMAE en algunas tareas.                                | Requiere investigación adicional sobre la influencia del tamaño del parche en el rendimiento.               | Alta: aplicable al mapeo de cobertura del suelo, monitoreo urbano y análisis de imágenes satelitales en Chile y LatAm.          |


### Revisión de Artículos importantes


Para cada estudio, incluir las siguientes subsecciones:

-   \[Título del Estudio\] Referencia completa: Autores, título, publicación, año.
-   Objetivo del estudio: ¿Qué problema aborda el artículo?
-   Métodos utilizados: Arquitecturas, datasets, técnicas de preentrenamiento.
-   Resultados principales: Métricas, tareas downstream, ventajas frente a otros enfoques.
-   Limitaciones identificadas: Críticas o desafíos no resueltos.
-   Relación con otros estudios: Comparación o integración con investigaciones similares.

*Nota: Usa tablas o gráficos para resumir comparaciones clave, como métricas de rendimiento en tareas downstream.*

## Discusión

### Análisis comparativo

-   Comparar las fortalezas y debilidades de los estudios analizados.
-   Identificación de patrones comunes en resultados o enfoques.

### Brechas en la literatura

-   Identificar áreas no exploradas o que requieren mayor desarrollo.

### Implicaciones para tu investigación

-   Cómo los resultados revisados fundamentan tu propuesta de tesis.
-   Adaptaciones o mejoras sugeridas para futuros trabajos.

## Conclusión y Propuesta

### Conclusión de la revisión

-   Síntesis de los hallazgos más importantes.

### Propuesta de tesis

-   Presentar la pregunta o hipótesis principal.
-   Breve descripción del enfoque metodológico y su relación con la revisión.

## Base General

| Abbreviation | Title | Publication | Paper | Code & Weights |
|:-------------:|---------------|:-------------:|:-------------:|:-------------:|
| **GeoKR** | **Geographical Knowledge-Driven Representation Learning for Remote Sensing Images** | TGRS2021 | [GeoKR](https://ieeexplore.ieee.org/abstract/document/9559903) | [link](https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning) |
| **-** | **Self-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding** | CVPRW2021 | [Paper](https://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Stojnic_Self-Supervised_Learning_of_Remote_Sensing_Scene_Representations_Using_Contrastive_Multiview_CVPRW_2021_paper.html) | [link](https://github.com/vladan-stojnic/CMC-RSSR) |
| **GASSL** | **Geography-Aware Self-Supervised Learning** | ICCV2021 | [GASSL](https://openaccess.thecvf.com/content/ICCV2021/html/Ayush_Geography-Aware_Self-Supervised_Learning_ICCV_2021_paper.html) | [link](https://github.com/sustainlab-group/geography-aware-ssl) |
| **SeCo** | **Seasonal Contrast: Unsupervised Pre-Training From Uncurated Remote Sensing Data** | ICCV2021 | [SeCo](https://openaccess.thecvf.com/content/ICCV2021/html/Manas_Seasonal_Contrast_Unsupervised_Pre-Training_From_Uncurated_Remote_Sensing_Data_ICCV_2021_paper.html) | [link](https://github.com/ServiceNow/seasonal-contrast) |
| **DINO-MM** | **Self-supervised Vision Transformers for Joint SAR-optical Representation Learning** | IGARSS2022 | [DINO-MM](https://arxiv.org/abs/2204.05381) | [link](https://github.com/zhu-xlab/DINO-MM) |
| **SatMAE** | **SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery** | NeurIPS2022 | [SatMAE](https://proceedings.neurips.cc/paper_files/paper/2022/hash/01c561df365429f33fcd7a7faa44c985-Abstract-Conference.html) | [link](https://github.com/sustainlab-group/SatMAE) |
| **RS-BYOL** | **Self-Supervised Learning for Invariant Representations From Multi-Spectral and SAR Images** | JSTARS2022 | [RS-BYOL](https://ieeexplore.ieee.org/abstract/document/9880533) | null |
| **GeCo** | **Geographical Supervision Correction for Remote Sensing Representation Learning** | TGRS2022 | [GeCo](https://ieeexplore.ieee.org/abstract/document/9869651) | null |
| **RingMo** | **RingMo: A remote sensing foundation model with masked image modeling** | TGRS2022 | [RingMo](https://ieeexplore.ieee.org/abstract/document/9844015) | [Code](https://github.com/comeony/RingMo) |
| **RVSA** | **Advancing plain vision transformer toward remote sensing foundation model** | TGRS2022 | [RVSA](https://ieeexplore.ieee.org/abstract/document/9956816) | [link](https://github.com/ViTAE-Transformer/Remote-Sensing-RVSA) |
| **RSP** | **An Empirical Study of Remote Sensing Pretraining** | TGRS2022 | [RSP](https://ieeexplore.ieee.org/abstract/document/9782149) | [link](https://github.com/ViTAE-Transformer/Remote-Sensing-RVSA) |
| **MATTER** | **Self-Supervised Material and Texture Representation Learning for Remote Sensing Tasks** | CVPR2022 | [MATTER](https://openaccess.thecvf.com/content/CVPR2022/html/Akiva_Self-Supervised_Material_and_Texture_Representation_Learning_for_Remote_Sensing_Tasks_CVPR_2022_paper.html) | null |
| **CSPT** | **Consecutive Pre-Training: A Knowledge Transfer Learning Strategy with Relevant Unlabeled Data for Remote Sensing Domain** | RS2022 | [CSPT](https://www.mdpi.com/2072-4292/14/22/5675#) | [link](https://github.com/ZhAnGToNG1/transfer_learning_cspt) |
| **-** | **Self-supervised Vision Transformers for Land-cover Segmentation and Classification** | CVPRW2022 | [Paper](https://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Scheibenreif_Self-Supervised_Vision_Transformers_for_Land-Cover_Segmentation_and_Classification_CVPRW_2022_paper.html) | [link](https://github.com/HSG-AIML/SSLTransformerRS) |
| **BFM** | **A billion-scale foundation model for remote sensing images** | Arxiv2023 | [BFM](https://arxiv.org/abs/2304.05215) | null |
| **TOV** | **TOV: The original vision model for optical remote sensing image understanding via self-supervised learning** | JSTARS2023 | [TOV](https://ieeexplore.ieee.org/abstract/document/10110958) | [link](https://github.com/GeoX-Lab/G-RSIM/tree/main/TOV_v1) |
| **CMID** | **CMID: A Unified Self-Supervised Learning Framework for Remote Sensing Image Understanding** | TGRS2023 | [CMID](https://ieeexplore.ieee.org/abstract/document/10105625) | [link](https://github.com/NJU-LHRS/official-CMID) |
| **RingMo-Sense** | **RingMo-Sense: Remote Sensing Foundation Model for Spatiotemporal Prediction via Spatiotemporal Evolution Disentangling** | TGRS2023 | [RingMo-Sense](https://ieeexplore.ieee.org/abstract/document/10254320) | null |
| **IaI-SimCLR** | **Multi-Modal Multi-Objective Contrastive Learning for Sentinel-1/2 Imagery** | CVPRW2023 | [IaI-SimCLR](https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Prexl_Multi-Modal_Multi-Objective_Contrastive_Learning_for_Sentinel-12_Imagery_CVPRW_2023_paper.html) | null |
| **CACo** | **Change-Aware Sampling and Contrastive Learning for Satellite Images** | CVPR2023 | [CACo](https://openaccess.thecvf.com/content/CVPR2023/html/Mall_Change-Aware_Sampling_and_Contrastive_Learning_for_Satellite_Images_CVPR_2023_paper.html) | [link](https://github.com/utkarshmall13/CACo) |
| **SatLas** | **SatlasPretrain: A Large-Scale Dataset for Remote Sensing Image Understanding** | ICCV2023 | [SatLas](https://arxiv.org/abs/2211.15660) | [link](https://github.com/allenai/satlas) |
| **GFM** | **Towards Geospatial Foundation Models via Continual Pretraining** | ICCV2023 | [GFM](https://arxiv.org/abs/2302.04476) | [link](https://github.com/mmendiet/GFM) |
| **Scale-MAE** | **Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning** | ICCV2023 | [Scale-MAE](https://arxiv.org/abs/2212.14532) | [link](https://github.com/bair-climate-initiative/scale-mae) |
| **DINO-MC** | **DINO-MC: Self-supervised Contrastive Learning for Remote Sensing Imagery with Multi-sized Local Crops** | Arxiv2023 | [DINO-MC](https://arxiv.org/abs/2303.06670) | [link](https://github.com/WennyXY/DINO-MC) |
| **CROMA** | **CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders** | NeurIPS2023 | [CROMA](https://arxiv.org/pdf/2311.00566.pdf) | [link](https://github.com/antofuller/CROMA) |
| **Cross-Scale MAE** | **Cross-Scale MAE: A Tale of Multiscale Exploitation in Remote Sensing** | NeurIPS2023 | [Cross-Scale MAE](https://openreview.net/pdf?id=5oEVdOd6TV) | [link](https://github.com/aicip/Cross-Scale-MAE) |
| **DeCUR** | **DeCUR: decoupling common & unique representations for multimodal self-supervision** | ECCV2024 | [DeCUR](https://arxiv.org/abs/2309.05300) | [link](https://github.com/zhu-xlab/DeCUR) |
| **Presto** | **Lightweight, Pre-trained Transformers for Remote Sensing Timeseries** | Arxiv2023 | [Presto](https://arxiv.org/abs/2304.14065) | [link](https://github.com/nasaharvest/presto) |
| **CtxMIM** | **CtxMIM: Context-Enhanced Masked Image Modeling for Remote Sensing Image Understanding** | Arxiv2023 | [CtxMIM](https://arxiv.org/abs/2310.00022) | null |
| **FG-MAE** | **Feature Guided Masked Autoencoder for Self-supervised Learning in Remote Sensing** | Arxiv2023 | [FG-MAE](https://arxiv.org/abs/2310.18653) | [link](https://github.com/zhu-xlab/FGMAE) |
| **Prithvi** | **Foundation Models for Generalist Geospatial Artificial Intelligence** | Arxiv2023 | [Prithvi](https://arxiv.org/abs/2310.18660) | [link](https://huggingface.co/ibm-nasa-geospatial) |
| **RingMo-lite** | **RingMo-lite: A Remote Sensing Multi-task Lightweight Network with CNN-Transformer Hybrid Framework** | Arxiv2023 | [RingMo-lite](https://arxiv.org/abs/2309.09003) | null |
| **-** | **A Self-Supervised Cross-Modal Remote Sensing Foundation Model with Multi-Domain Representation and Cross-Domain Fusion** | IGARSS2023 | [Paper](https://ieeexplore.ieee.org/abstract/document/10282433) | null |
| **EarthPT** | **EarthPT: a foundation model for Earth Observation** | NeurIPS2023 CCAI workshop | [EarthPT](https://arxiv.org/abs/2309.07207) | [link](https://github.com/aspiaspace/EarthPT) |
| **USat** | **USat: A Unified Self-Supervised Encoder for Multi-Sensor Satellite Imagery** | Arxiv2023 | [USat](https://arxiv.org/abs/2312.02199) | [link](https://github.com/stanfordmlgroup/USat) |
| **FoMo-Bench** | **FoMo-Bench: a multi-modal, multi-scale and multi-task Forest Monitoring Benchmark for remote sensing foundation models** | Arxiv2023 | [FoMo-Bench](https://arxiv.org/abs/2312.10114) | [link](https://github.com/RolnickLab/FoMo-Bench) |
| **AIEarth** | **Analytical Insight of Earth: A Cloud-Platform of Intelligent Computing for Geospatial Big Data** | Arxiv2023 | [AIEarth](https://arxiv.org/abs/2312.16385) | [link](https://engine-aiearth.aliyun.com/#/) |
| **-** | **Self-Supervised Learning for SAR ATR with a Knowledge-Guided Predictive Architecture** | Arxiv2023 | [Paper](https://arxiv.org/abs/2311.15153v4) | [link](https://github.com/waterdisappear/SAR-JEPA) |
| **Clay** | **Clay Foundation Model** | \- | null | [link](https://clay-foundation.github.io/model/) |
| **Hydro** | **Hydro--A Foundation Model for Water in Satellite Imagery** | \- | null | [link](https://github.com/isaaccorley/hydro-foundation-model) |
| **U-BARN** | **Self-Supervised Spatio-Temporal Representation Learning of Satellite Image Time Series** | JSTARS2024 | [Paper](https://ieeexplore.ieee.org/document/10414422) | [link](https://src.koda.cnrs.fr/iris.dumeur/ssl_ubarn) |
| **GeRSP** | **Generic Knowledge Boosted Pre-training For Remote Sensing Images** | Arxiv2024 | [GeRSP](https://arxiv.org/abs/2401.04614) | [GeRSP](https://github.com/floatingstarZ/GeRSP) |
| **SwiMDiff** | **SwiMDiff: Scene-wide Matching Contrastive Learning with Diffusion Constraint for Remote Sensing Image** | Arxiv2024 | [SwiMDiff](https://arxiv.org/abs/2401.05093) | null |
| **OFA-Net** | **One for All: Toward Unified Foundation Models for Earth Vision** | Arxiv2024 | [OFA-Net](https://arxiv.org/abs/2401.07527) | null |
| **SMLFR** | **Generative ConvNet Foundation Model With Sparse Modeling and Low-Frequency Reconstruction for Remote Sensing Image Interpretation** | TGRS2024 | [SMLFR](https://ieeexplore.ieee.org/abstract/document/10378718) | [link](https://github.com/HIT-SIRS/SMLFR) |
| **SpectralGPT** | **SpectralGPT: Spectral Foundation Model** | TPAMI2024 | [SpectralGPT](https://arxiv.org/abs/2311.07113) | [link](https://github.com/danfenghong/IEEE_TPAMI_SpectralGPT) |
| **S2MAE** | **S2MAE: A Spatial-Spectral Pretraining Foundation Model for Spectral Remote Sensing Data** | CVPR2024 | [S2MAE](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_S2MAE_A_Spatial-Spectral_Pretraining_Foundation_Model_for_Spectral_Remote_Sensing_CVPR_2024_paper.pdf) | null |
| **SatMAE++** | **Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery** | CVPR2024 | [SatMAE++](https://arxiv.org/abs/2403.05419) | [link](https://github.com/techmn/satmae_pp) |
| **msGFM** | **Bridging Remote Sensors with Multisensor Geospatial Foundation Models** | CVPR2024 | [msGFM](https://arxiv.org/abs/2404.01260) | [link](https://github.com/boranhan/Geospatial_Foundation_Models) |
| **SkySense** | **SkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal Interpretation for Earth Observation Imagery** | CVPR2024 | [SkySense](https://arxiv.org/abs/2312.10115) | Targeted open-source |
| **MTP** | **MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining** | Arxiv2024 | [MTP](https://arxiv.org/abs/2403.13430) | [link](https://github.com/ViTAE-Transformer/MTP) |
| **DOFA** | **Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities** | Arxiv2024 | [DOFA](https://arxiv.org/abs/2403.15356) | [link](https://github.com/zhu-xlab/DOFA) |
| **MMEarth** | **MMEarth: Exploring Multi-Modal Pretext Tasks For Geospatial Representation Learning** | ECCV2024 | [MMEarth](https://arxiv.org/abs/2405.02771) | [link](https://vishalned.github.io/mmearth/) |
| **SARATR-X** | **SARATR-X: A Foundation Model for Synthetic Aperture Radar Images Target Recognition** | Arxiv2024 | [SARATR-X](https://arxiv.org/abs/2405.09365) | [link](https://github.com/waterdisappear/SARATR-X) |
| **LeMeViT** | **LeMeViT: Efficient Vision Transformer with Learnable Meta Tokens for Remote Sensing Image Interpretation** | IJCAI2024 | [LeMeViT](https://arxiv.org/abs/2405.09789) | [link](https://github.com/ViTAE-Transformer/LeMeViT/tree/main?tab=readme-ov-file) |
| **SoftCon** | **Multi-Label Guided Soft Contrastive Learning for Efficient Earth Observation Pretraining** | Arxiv2024 | [SoftCon](https://arxiv.org/abs/2405.20462) | [link](https://github.com/zhu-xlab/softcon?tab=readme-ov-file) |
| **RS-DFM** | **RS-DFM: A Remote Sensing Distributed Foundation Model for Diverse Downstream Tasks** | Arxiv2024 | [RS-DFM](https://arxiv.org/abs/2406.07032) | null |
| **A2-MAE** | **A2-MAE: A spatial-temporal-spectral unified remote sensing pre-training method based on anchor-aware masked autoencoder** | Arxiv2024 | [A2-MAE](https://arxiv.org/abs/2406.08079) | null |
| **HyperSIGMA** | **HyperSIGMA: Hyperspectral Intelligence Comprehension Foundation Model** | Arxiv2024 | [HyperSIGMA](https://arxiv.org/abs/2406.11519) | [link](https://github.com/WHU-Sigma/HyperSIGMA?tab=readme-ov-file) |
| **SelectiveMAE** | **Scaling Efficient Masked Autoencoder Learning on Large Remote Sensing Dataset** | Arxiv2024 | [SelectiveMAE](https://arxiv.org/abs/2406.11933) | [link](https://github.com/Fengxiang23/SelectiveMAE) |
| **OmniSat** | **OmniSat: Self-Supervised Modality Fusion for Earth Observation** | ECCV2024 | [OmniSat](https://arxiv.org/pdf/2404.08351) | [link](https://github.com/gastruc/OmniSat?tab=readme-ov-file) |
| **MM-VSF** | **Towards a Knowledge guided Multimodal Foundation Model for Spatio-Temporal Remote Sensing Applications** | Arxiv2024 | [MM-VSF](https://arxiv.org/pdf/2407.19660) | null |
| **MA3E** | **Masked Angle-Aware Autoencoder for Remote Sensing Images** | ECCV2024 | [MA3E](https://arxiv.org/abs/2408.01946) | [link](https://github.com/benesakitam/MA3E) |
| **SpectralEarth** | **SpectralEarth: Training Hyperspectral Foundation Models at Scale** | Arxiv2024 | [SpectralEarth](https://arxiv.org/abs/2408.08447) | null |
| **SenPa-MAE** | **SenPa-MAE: Sensor Parameter Aware Masked Autoencoder for Multi-Satellite Self-Supervised Pretraining** | Arxiv2024 | [SenPa-MAE](https://arxiv.org/abs/2408.11000) | [link](https://github.com/JonathanPrexl/SenPa-MAE) |
| **RingMo-Aerial** | **RingMo-Aerial: An Aerial Remote Sensing Foundation Model With A Affine Transformation Contrastive Learning** | Arxiv2024 | [RingMo-Aerial](https://arxiv.org/abs/2409.13366) | null |
| **SAR-JEPA** | **Predicting Gradient is Better: Exploring Self-Supervised Learning for SAR ATR with a Joint-Embedding Predictive Architecture** | ISPRS JPRS2024 | [SAR-JEPA](https://www.sciencedirect.com/science/article/pii/S0924271624003514) | [link](https://github.com/waterdisappear/SAR-JEPA) |
| **PIS** | **Pretrain a Remote Sensing Foundation Model by Promoting Intra-instance Similarity** | TGRS2024 | [PIS](https://ieeexplore.ieee.org/abstract/document/10697182) | [link](https://github.com/ShawnAn-WHU/PIS) |
| **OReole-FM** | **OReole-FM: successes and challenges toward billion-parameter foundation models for high-resolution satellite imagery** | SIGSPATIAL2024 | [OReole-FM](https://arxiv.org/abs/2410.19965) | null |
| **PIEViT** | **Pattern Integration and Enhancement Vision Transformer for Self-supervised Learning in Remote Sensing** | Arxiv2024 | [PIEViT](https://arxiv.org/pdf/2411.06091) | null |
| **SatVision-TOA** | **SatVision-TOA: A Geospatial Foundation Model for Coarse-Resolution All-Sky Remote Sensing Imagery** | Arxiv2024 | [SatVision-TOA](https://arxiv.org/abs/2411.17000) | [link](https://github.com/nasa-nccs-hpda/pytorch-caney) |
| **RS-vHeat** | **RS-vHeat: Heat Conduction Guided Efficient Remote Sensing Foundation Model** | Arxiv2024 | [RS-vHeat](https://arxiv.org/abs/2411.17984) | null |
| **Prithvi-EO-2.0** | **Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications** | Arxiv2024 | [Prithvi-EO-2.0](https://arxiv.org/abs/2412.02732) | [link](https://github.com/NASA-IMPACT/Prithvi-EO-2.0) |
| **AnySat** | **AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities** | Arxiv2024 | [AnySat](https://arxiv.org/abs/2412.14123) | [link](https://github.com/gastruc/AnySat) |
