[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelos Fundacionales de IA",
    "section": "",
    "text": "Preface\n“Revisión de Modelos Fundacionales en Remote Sensing: Avances, Desafíos y Aplicaciones”\nEl propósito de este libro/review es proporcionar una visión integral y actualizada del desarrollo y aplicación de los modelos fundacionales en el área de Remote Sensing. Se analiza cómo estos modelos, a través de técnicas de pre-entrenamiento en grandes volúmenes de datos y ajuste fino en tareas específicas, han revolucionado la interpretación y análisis de datos geoespaciales. Además, se identifican los principales desafíos, se destacan los avances recientes y se proponen direcciones futuras para optimizar el uso de estos modelos en la comunidad científica y profesional.\nLa motivación central de este review surge de la creciente complejidad y heterogeneidad de los datos en Remote Sensing, que demandan enfoques robustos, escalables y generalizables. A medida que las tecnologías de sensores avanzan, se generan volúmenes masivos de información multiespectral, temporal y espacial, lo que desafía los enfoques tradicionales y requiere la implementación de modelos más sofisticados y eficientes.\nEl presente trabajo tiene como objetivo analizar los avances más recientes en modelos fundacionales aplicados a Remote Sensing, identificar sus aplicaciones clave y explorar los desafíos y oportunidades asociados a su implementación. Este estudio proporciona una perspectiva integral que facilita la comprensión del impacto de estos modelos en el área y establece las bases para futuras investigaciones y desarrollos.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#autor",
    "href": "index.html#autor",
    "title": "Modelos Fundacionales de IA",
    "section": "Autor:",
    "text": "Autor:\nDenis Berroeta\nweb: web personal, email: denisberroeta@gmail.com\nMaster of Science in Data Science, Magíster en Inteligencia Artificial y cursando un Doctorado en Data Science. Ingeniero en Prevención de Riesgos y Topógrafo. Temas de interés Inteligencia Artificial (LLM, CNN), Análisis espacial, Percepción Remota.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Conceptos de Modelos Fundacionales\nEl Remote Sensing juega un papel fundamental en la observación y análisis de la Tierra, permitiendo el monitoreo continuo y detallado de nuestro planeta a través de tecnologías satelitales y aéreas. Desde la detección de cambios ambientales hasta la gestión de desastres naturales, su relevancia se ha incrementado exponencialmente en los últimos años gracias a la creciente disponibilidad de datos de alta resolución y la evolución de los métodos de procesamiento.\nEn este contexto, los modelos fundacionales (FMs) representan una innovación significativa. Estos modelos pre-entrenados, capaces de aprender representaciones generales a partir de grandes volúmenes de datos no etiquetados, permiten una adaptación eficiente a tareas específicas con la necesidad de pocos datos anotados. La versatilidad de los FMs radica en su capacidad de abordar múltiples aplicaciones en Remote Sensing, como la segmentación de imágenes, detección de objetos y clasificación de escenas, optimizando así el análisis de datos complejos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#conceptos-de-modelos-fundacionales",
    "href": "intro.html#conceptos-de-modelos-fundacionales",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 Modelo fundacional (MF)\nUn modelo fundacional se define como un modelo de inteligencia artificial pre-entrenado en grandes volúmenes de datos no etiquetados que puede ser ajustado (fine-tuned) a una variedad de tareas específicas con un esfuerzo computacional significativamente menor. Estos modelos han demostrado capacidades avanzadas de generalización, adaptabilidad y eficiencia en dominios complejos como el Remote Sensing (Xiao et al. 2024).\nEn el ámbito de Remote Sensing, los modelos fundacionales aprovechan datos multiespectrales, temporales y de alta resolución espacial obtenidos de sensores satelitales y aéreos. Su aplicación permite resolver tareas críticas como la clasificación de escenas, segmentación semántica, detección de objetos y detección de cambios con alta precisión y eficiencia computacional (Jakubik et al. 2023).\n\n\n1.1.2 Pre-entrenamiento\nEl pre-entrenamiento es el proceso mediante el cual un modelo aprende representaciones generales del dominio a partir de grandes volúmenes de datos no etiquetados. Esta etapa utiliza técnicas de aprendizaje auto-supervisado (SSL) como el contrastive learning y el masked autoencoding, permitiendo al modelo captar patrones, relaciones y estructuras en los datos sin intervención manual en el etiquetado (Lu et al., 2024). En el caso del Remote Sensing, el pre-entrenamiento se lleva a cabo sobre datos satelitales masivos como Sentinel-2, BigEarthNet o HLS (Jakubik et al. 2023).\n\n\n1.1.3 Fine-Tuning\nEl fine-tuning consiste en ajustar un modelo fundacional pre-entrenado a una tarea específica utilizando un conjunto de datos etiquetados más reducido. Durante esta etapa, el modelo reutiliza las representaciones generales aprendidas en el pre-entrenamiento y las optimiza para una aplicación concreta, como la segmentación de imágenes o la detección de objetos. Este enfoque mejora la eficiencia del modelo al reducir la necesidad de grandes cantidades de datos etiquetados y recursos computacionales (Xiao et al. 2024).\nEn resumen, los modelos fundacionales revolucionan el Remote Sensing al permitir la aplicación eficiente de técnicas avanzadas de inteligencia artificial en tareas complejas y multiespectrales, facilitando el análisis de datos geoespaciales a escala global.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#tipos-de-modelos-fundacionales",
    "href": "intro.html#tipos-de-modelos-fundacionales",
    "title": "1  Introduction",
    "section": "1.2 Tipos de Modelos Fundacionales",
    "text": "1.2 Tipos de Modelos Fundacionales\nVisual Foundation Models (VFMs): Estos modelos se centran en el procesamiento y análisis de datos visuales, abordando tareas como segmentación semántica, detección de objetos y clasificación de escenas. Utilizan arquitecturas avanzadas como Vision Transformers (ViT) y enfoques auto-supervisados para maximizar la eficiencia en escenarios con datos multiespectrales y de alta resolución (Jakubik et al. 2023), (Lu et al. 2024).\n\nVision-Language Models (VLMs): Diseñados para integrar datos visuales y textuales, estos modelos combinan imágenes satelitales con descripciones textuales para realizar tareas como visual grounding, generación de descripciones automáticas y respuestas a preguntas basadas en imágenes. Su capacidad multimodal permite aplicaciones más intuitivas y precisas en el campo del Remote Sensing (Xiao et al. 2024) .",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#arquitecturas-comunes",
    "href": "intro.html#arquitecturas-comunes",
    "title": "1  Introduction",
    "section": "1.3 Arquitecturas Comunes",
    "text": "1.3 Arquitecturas Comunes\nConvolutional Neural Networks (CNNs): Estas redes son ampliamente utilizadas por su capacidad para extraer características jerárquicas de imágenes a través de operaciones convolucionales. En Remote Sensing, han demostrado eficacia en tareas como clasificación de imágenes y detección de objetos, pero enfrentan limitaciones en la captura de dependencias globales en imágenes de alta resolución (Lu et al. 2024).\nVision Transformers (ViT): Los Transformers son arquitecturas basadas en mecanismos de self-attention que permiten modelar dependencias de largo alcance en imágenes dividiéndolas en parches tratados como secuencias. Su flexibilidad y precisión los hacen ideales para tareas como la segmentación semántica en datos multiespectrales y multitemporales (Jakubik et al., 2023).\n\n\n\nViT.png\n\n\nMasked Autoencoder (MAE): Este modelo utiliza el enfoque de reconstrucción de imágenes donde una fracción significativa de los píxeles se enmascara durante el entrenamiento, y el modelo aprende a predecir las partes faltantes. En Remote Sensing, los MAE son efectivos para captar patrones en datos satelitales multiespectrales y temporales, optimizando la eficiencia en tareas de imputación y análisis de imágenes incompletas (Xiao et al. 2024).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#datasets-en-remote-sensing",
    "href": "intro.html#datasets-en-remote-sensing",
    "title": "1  Introduction",
    "section": "1.4 5. Datasets en Remote Sensing",
    "text": "1.4 5. Datasets en Remote Sensing\n\n\n\n\n\n\n\n\n\n\nNombre del Dataset\nTamaño\nResolución\nCaracterísticas Clave\nPaper de Referencia\n\n\n\n\nSentinel-1/2\n1M+ imágenes\n10-30m\nMultiespectral y temporal\n(Xiao et al. 2024)\n\n\nHarmonized Landsat-Sentinel (HLS)\n1TB\n30m\nImagen multitemporal y multiespectral\n(Jakubik et al. 2023)\n\n\nBigEarthNet\n590K+\n10-60m\nCobertura en Europa\n(Lu et al. 2024)\n\n\nSSL4EO-S12\n3M+\n10-60m\nMultimodal (SAR + óptico)\n(Xiao et al. 2024)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#metodologías-de-pre-entrenamiento",
    "href": "intro.html#metodologías-de-pre-entrenamiento",
    "title": "1  Introduction",
    "section": "1.5 Metodologías de Pre-Entrenamiento",
    "text": "1.5 Metodologías de Pre-Entrenamiento\n\n1.5.1 Supervised Learning\nEl pre-entrenamiento con datos etiquetados, como en el caso del dataset MillionAID, consiste en entrenar modelos utilizando grandes volúmenes de datos previamente anotados con etiquetas precisas. Este enfoque facilita la capacidad del modelo para capturar representaciones complejas y específicas del dominio, lo que resulta particularmente efectivo en tareas como la clasificación de escenas y la detección de objetos en Remote Sensing (Lu et al. 2024). En este contexto, MillionAID se ha convertido en un recurso clave al proporcionar un conjunto diverso de imágenes que abordan múltiples categorías globales, contribuyendo significativamente al desarrollo de modelos robustos y generalizables (Xiao et al. 2024).\n\n\n1.5.2 Self-Supervised Learning (SSL)\nContrastive Learning: Este enfoque utiliza pares positivos y negativos para maximizar la similitud entre representaciones de datos similares y minimizarla entre diferentes. Los pares positivos se generan generalmente aplicando transformaciones a la misma muestra, como recortes, rotaciones o cambios de escala, mientras que los pares negativos provienen de diferentes muestras dentro del lote de datos. Este mecanismo fuerza al modelo a aprender representaciones invariantes a estas transformaciones, lo que es crucial en datos multitemporales. Por ejemplo, SeCo emplea aprendizaje contrastivo para capturar invariancias temporales en datos de observación terrestre, mientras que GASSL integra geolocalización y aprendizaje temporal para enriquecer las representaciones, aprovechando datos satelitales masivos (Xiao et al. 2024).\nMasked Autoencoding: En este método, el modelo aprende a reconstruir las partes faltantes de una entrada, lo que mejora su comprensión de estructuras globales. SatMAE es un ejemplo clave que se enfoca en datos multiespectrales y multitemporales, mientras que Scale-MAE incorpora información de diferentes escalas para optimizar el aprendizaje (Jakubik et al. 2023).\nMultimodal Pre-training: Este enfoque integra diferentes tipos de datos, como SAR, ópticos y auxiliares, para mejorar la capacidad del modelo en tareas complejas. La combinación de modalidades permite al modelo captar patrones complementarios y abordar la heterogeneidad en datos de Remote Sensing (Lu et al. 2024).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#aplicaciones-en-tareas-de-remote-sensing",
    "href": "intro.html#aplicaciones-en-tareas-de-remote-sensing",
    "title": "1  Introduction",
    "section": "1.6 Aplicaciones en Tareas de Remote Sensing",
    "text": "1.6 Aplicaciones en Tareas de Remote Sensing\n\n\n\nFM_tasks.png\n\n\n\n1.6.1 Clasificación de Escenas\nLa clasificación de escenas es el proceso mediante el cual se categorizan imágenes satelitales completas en clases predefinidas, como áreas urbanas, forestales, cuerpos de agua o agrícolas. Este enfoque permite extraer información útil para tareas como la gestión de recursos naturales, el monitoreo ambiental y la planificación urbana. Modelos como GeoKR y CSPT son ejemplos destacados que emplean arquitecturas avanzadas para mejorar la precisión y escalabilidad en este tipo de tareas.\n\n\n1.6.2 Segmentación Semántica\nLa segmentación semántica es una tarea crítica en Remote Sensing que asigna una etiqueta a cada píxel de una imagen, permitiendo la identificación precisa de diferentes tipos de cobertura terrestre como agua, bosques y áreas urbanas. Este nivel de detalle es esencial para aplicaciones como la gestión ambiental, la planificación urbana y la agricultura de precisión. Por ejemplo, Prithvi ha demostrado ser efectivo en tareas de segmentación multitemporal, aprovechando datos multiespectrales para mapear dinámicamente cambios en el uso de la tierra. De manera similar, SatMAE++ utiliza enfoques avanzados de auto-codificación para mejorar la segmentación en imágenes con variabilidad de escalas, logrando una alta precisión incluso en escenarios complejos (Jakubik et al. 2023), (Xiao et al. 2024).\n\n\n1.6.3 Detección de Objetos\nLa detección de objetos es un proceso en el cual se identifican y localizan entidades específicas dentro de imágenes satelitales, como edificios, vehículos o embarcaciones. Este enfoque es fundamental para aplicaciones como el monitoreo urbano, la gestión de desastres y la supervisión de infraestructuras críticas. Por ejemplo, RingMo utiliza enfoques avanzados de aprendizaje multimodal para mejorar la detección en imágenes multiespectrales y multitemporales. Por otro lado, SkySense combina datos ópticos y SAR para abordar desafíos relacionados con variaciones ambientales y características espectrales complejas, proporcionando una mayor robustez y precisión en tareas de detección (Xiao et al. 2024), (Lu et al. 2024)\n\n\n1.6.4 Detección de Cambios\nLa detección de cambios implica el uso de imágenes multitemporales para identificar y analizar alteraciones en la superficie terrestre a lo largo del tiempo. Este proceso es fundamental para aplicaciones como el monitoreo de deforestación, expansiones urbanas y desastres naturales. Modelos como GeoChange y ChangeSTAR se utilizan para procesar grandes volúmenes de datos satelitales, integrando algoritmos que permiten una comparación precisa entre diferentes momentos temporales, destacando incluso los cambios más sutiles ((Jakubik et al. 2023), (Xiao et al. 2024)).\n\n\n1.6.5 Imputación de Nubes\nEl proceso de imputación de nubes, conocido como cloud gap filling, se refiere a la tarea de reconstruir áreas de datos faltantes en imágenes satelitales debido a la presencia de nubes. Este problema es crítico para garantizar la continuidad espacial y temporal de los datos en aplicaciones como el monitoreo de cultivos o el análisis ambiental. Por ejemplo, Prithvi emplea algoritmos basados en modelos fundacionales para rellenar estos vacíos de manera precisa, utilizando datos multiespectrales y multitemporales como referencia contextual, lo que mejora significativamente la calidad de las observaciones satelitales (Xiao et al. 2024).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#desafíos-y-oportunidades",
    "href": "intro.html#desafíos-y-oportunidades",
    "title": "1  Introduction",
    "section": "1.7 Desafíos y Oportunidades",
    "text": "1.7 Desafíos y Oportunidades\n\n1.7.1 Desafíos\n\nEscasez de datos etiquetados para pre-entrenamiento.\nCostos computacionales: entrenamiento en grandes volúmenes de datos.\nGeneralización limitada en dominios multiespectrales y multimodales.\n\n\n\n1.7.2 Oportunidades\n\nIntegración de modelos multimodales (e.g., VLMs para texto-imagen).\nAvances en transferencia de conocimiento entre dominios geoespaciales.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#referencias",
    "href": "intro.html#referencias",
    "title": "1  Introduction",
    "section": "1.8 Referencias",
    "text": "1.8 Referencias\nJakubik et al., “Foundation Models for Generalist Geospatial AI”, 2023.\nXiao et al., “Foundation Models for Remote Sensing and Earth Observation: A Survey”, 2024.\nLu et al., “AI Foundation Models in Remote Sensing: A Survey”, 2024. Otros artículos relevantes. -\n\n\n\n\nJakubik, Johannes, Sujit Roy, C. E. Phillips, Paolo Fraccaro, Denys Godwin, Bianca Zadrozny, Daniela Szwarcman, et al. 2023. “Foundation Models for Generalist Geospatial Artificial Intelligence.” https://doi.org/10.48550/arXiv.2310.18660.\n\n\nLu, Siqi, Junlin Guo, James R. Zimmer-Dauphinee, Jordan M. Nieusma, Xiao Wang, Parker VanValkenburgh, Steven A. Wernke, and Yuankai Huo. 2024. “AI Foundation Models in Remote Sensing: A Survey.” https://doi.org/10.48550/arXiv.2408.03464.\n\n\nXiao, Aoran, Weihao Xuan, Junjue Wang, Jiaxing Huang, Dacheng Tao, Shijian Lu, and Naoto Yokoya. 2024. “Foundation Models for Remote Sensing and Earth Observation: A Survey.” https://doi.org/10.48550/arXiv.2410.16602.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "papers_review_vfm.html",
    "href": "papers_review_vfm.html",
    "title": "2  Papers Review: VFM",
    "section": "",
    "text": "2.1 Introducción",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Papers Review: VFM</span>"
    ]
  },
  {
    "objectID": "papers_review_vfm.html#introducción",
    "href": "papers_review_vfm.html#introducción",
    "title": "2  Papers Review: VFM",
    "section": "",
    "text": "2.1.1 Contexto general\nEl Remote Sensing (teledetección) ha emergido como una herramienta crucial para la observación y análisis del planeta Tierra, permitiendo monitorear fenómenos ambientales y territoriales con un nivel de detalle sin precedentes. Tecnologías como las imágenes satelitales y los sistemas aéreos han facilitado la recopilación continua de datos espaciales de alta resolución, lo que es fundamental para aplicaciones como la detección de cambios ambientales, la gestión de desastres naturales, el monitoreo de recursos y la planificación territorial.\nEn este contexto, los modelos fundacionales (Foundation Models, FMs) representan una innovación disruptiva en el campo de la inteligencia artificial aplicada a la percepción remota. Estos modelos preentrenados sobre grandes volúmenes de datos no etiquetados son capaces de aprender representaciones generales que pueden ajustarse fácilmente a tareas específicas utilizando cantidades relativamente pequeñas de datos anotados. Su flexibilidad ha permitido abordar problemas complejos en teledetección, como la segmentación semántica, la detección de objetos y la clasificación de escenas, optimizando así el análisis de datos multiespectrales y temporales.\nLos avances recientes en Visual Foundation Models (VFMs), basados en arquitecturas avanzadas como los Vision Transformers (ViT) y enfoques autosupervisados, han potenciado su aplicabilidad a datos de alta resolución. Este progreso se alinea con la creciente capacidad de procesamiento computacional y la disponibilidad masiva de imágenes satelitales de alta calidad, las cuales aumentan continuamente en resolución espacial y temporal. Estas características posicionan a los VFMs como una herramienta prometedora para enfrentar desafíos territoriales y ambientales, particularmente en regiones como Chile y América Latina, donde los datos geoespaciales son abundantes pero subutilizados.\n\n\n2.1.2 Objetivos de la revisión\nEl propósito de esta revisión bibliográfica es:\n\nExplorar el estado del arte en el desarrollo y aplicaciones de los modelos fundacionales en percepción remota, con un enfoque en los Visual Foundation Models (VFMs).\nAnalizar su potencial para abordar problemas territoriales específicos en contextos geográficos de Chile y América Latina.\nIdentificar oportunidades y desafíos para la implementación de estos modelos, considerando tanto la creciente disponibilidad de datos geoespaciales como las capacidades computacionales actuales.\nProponer una base conceptual que sirva como fundamento para evaluar y adaptar VFMs en aplicaciones concretas, maximizando su impacto en problemas como planificación territorial, monitoreo ambiental y manejo de desastres.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Papers Review: VFM</span>"
    ]
  },
  {
    "objectID": "papers_review_vfm.html#metodología",
    "href": "papers_review_vfm.html#metodología",
    "title": "2  Papers Review: VFM",
    "section": "2.2 Metodología",
    "text": "2.2 Metodología\n\n2.2.1 Criterios de Inclusión\nPara garantizar que los estudios seleccionados sean relevantes y alineados con los objetivos de esta revisión, se definirán los siguientes criterios de inclusión:\n\nRelevancia temática\n\n\nEstudios centrados en modelos fundacionales de visión (VFMs) aplicados a teledetección, específicamente en tareas como detección de cambios, clasificación de uso de suelo, segmetación semántica.\nInvestigaciones relacionadas con el uso de datos geoespaciales, imágenes satelitales o aéreas en problemas ambientales y sociales.\n\n\nReciente publicación\n\n\nPublicaciones desde 2020 en adelante, considerando la novedad del campo y su evolución rápida.\nPublicaciones en revistas indexadas o conferencias de alto impacto en áreas de inteligencia artificial, percepción remota o teledetección.\n\n\nRelevancia geográfica y temática\n\n\nEstudios que incluyan aplicaciones en problemas reales relacionados con el medio ambiente y las personas, como:\nCambio climático, gestión de recursos naturales, agricultura sostenible, planificación urbana, monitoreo de desastres o biodiversidad.\nPreferencia por estudios que incluyan contextos aplicables a Chile o América Latina, aunque no se excluirán estudios con enfoques globales relevantes.\n\n\nDisponibilidad de resultados, métricas y códigos\n\n\nEstudios que incluyan evaluaciones cuantitativas de desempeño, como métricas de clasificación, segmentación o detección (e.g., IoU, precisión, F1-score).\nDescripción detallada de los datasets utilizados y la metodología aplicada, eventualmente códigos.\n\n\nEnfoque en arquitectura y métodos de aprendizaje\n\n\nModelos basados en arquitecturas avanzadas, como Vision Transformers (ViT) y enfoques autosupervisados (e.g., MAE, contrastive learning).\nInnovaciones metodológicas para optimizar el uso de datos no etiquetados o resolver problemas multiespectrales y multitemporales.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Papers Review: VFM</span>"
    ]
  },
  {
    "objectID": "papers_review_vfm.html#marco-conceptual",
    "href": "papers_review_vfm.html#marco-conceptual",
    "title": "2  Papers Review: VFM",
    "section": "2.3 Marco Conceptual",
    "text": "2.3 Marco Conceptual\nA continuación, se organiza y define cada concepto clave relevante al tema de la revisión bibliográfica sobre modelos fundacionales de visión aplicados a teledetección. Estos conceptos proporcionan el fundamento teórico necesario para comprender el desarrollo y aplicación de los Visual Foundation Models (VFMs) en problemas territoriales y ambientales.\n\nSon modelos de aprendizaje profundo preentrenados sobre grandes volúmenes de datos no etiquetados, diseñados para tareas de visión por computadora. Su arquitectura avanzada, basada en enfoques como Vision Transformers (ViT), los hace altamente adaptables a una variedad de aplicaciones en teledetección, desde segmentación semántica hasta detección de cambios.\n\n2.3.1 Autoencoders Enmascarados (MAE)\nLos MAE son modelos de aprendizaje profundo diseñados para reconstruir datos de entrada a partir de una versión parcialmente enmascarada.\n\nAplicación en VFMs: En el contexto de la teledetección, los MAE son clave para procesar imágenes satelitales, permitiendo al modelo aprender representaciones robustas incluso con datos incompletos o ruidosos.\n\n\n\n2.3.2 Aprendizaje Contrastivo\nEs una técnica de aprendizaje autosupervisado que entrena al modelo para distinguir entre pares de datos similares (positivos) y disímiles (negativos).\n\nRelevancia en VFMs: Es utilizado en modelos como CROMA para aprender representaciones invariantes a sensores múltiples (e.g., radar y ópticos).\n\n\n\n2.3.3 Datos Multimodales y Alineación Espacial\n\nDatos Multimodales: Provienen de sensores diferentes (e.g., Sentinel-1 para radar y Sentinel-2 para óptico), enriqueciendo las representaciones aprendidas por los VFMs.\nAlineación Espacial: Garantiza que los datos de distintos sensores correspondan a la misma ubicación geográfica, asegurando consistencia en las representaciones aprendidas.\n\n\n\n2.3.4 Codificación de Posición Relativa (RPE)\nTécnica utilizada para incorporar información posicional en modelos basados en transformers. - Impacto en VFMs: Mejora la capacidad de generalización de modelos como CROMA en imágenes más grandes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Papers Review: VFM</span>"
    ]
  },
  {
    "objectID": "papers_review_vfm.html#revisión-de-los-estudios",
    "href": "papers_review_vfm.html#revisión-de-los-estudios",
    "title": "2  Papers Review: VFM",
    "section": "2.4 Revisión de los Estudios",
    "text": "2.4 Revisión de los Estudios\nPara organizar y analizar los artículos científicos se construyó una tabla que resume información importante para cumplir con los objetivo del presente capítulo, los criterios y contenidos estatal corresponde a los siguientes:\n\nReferencia: Mencionar los autores y año, incluyendo una referencia breve para seguimiento.\nObjetivo del Estudio: Resumir el problema y la tarea específica abordada.\nArquitectura: Detallar el modelo utilizado (e.g., ViT, Swin Transformer, MAE).\nDataset: Especificar los datasets usados, incluyendo resolución y tipo (e.g., SAR, multiespectral).\nAplicación: Indicar cómo se aplica el modelo (e.g., monitoreo de inundaciones, detección de cultivos).\nMétricas: Proporcionar los valores clave de desempeño.\nLimitaciones: Identificar las barreras técnicas o de aplicación señaladas en el estudio.\nRelevancia para Chile/LatAm: Evaluar la aplicabilidad en contextos locales según problemas ambientales o sociales.\n\n\n2.4.1 Tabla Resumen General\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferencia\nObjetivo del Estudio\nArquitectura\nDataset\nAplicación\nMétricas\nLimitaciones\nRelevancia para Chile/LatAm\n\n\n\n\nFuller et al. (2023)\nDesarrollar un framework para aprender representaciones unimodales y multimodales para la Observación de la Tierra, aprovechando objetivos autosupervisados contrastivos y de reconstrucción.\nCROMA (autoencoder contrastivo radar-óptico enmascarado), basado en Vision Transformer (ViT), con codificación de posición 2D-ALiBi y X-ALiBi.\nBigEarthNet, fMoW-Sentinel, EuroSAT, Canadian Cropland, DFC2020, DW-Expert, MARIDA. Usando imágenes Sentinel-2 (óptico) y Sentinel-1 (radar) a diferentes resoluciones.\nClasificación de imágenes satelitales y segmentación semántica.\nSupera el estado del arte (SatMAE) en un promedio de 5.4% y 6.4% en mIoU para backbones ViT-B y ViT-L, respectivamente.\nRequiere datos multimodales alineados espacialmente. Alto costo computacional para entrenar modelos grandes.\nAlta: Chile posee una amplia cobertura de imágenes Sentinel, lo que permitiría aplicar CROMA para diversas aplicaciones como la clasificación de uso y cobertura del suelo, monitoreo de recursos hídricos, agricultura de precisión, entre otras.\n\n\nWang et al. (2022)\nProponer una red neuronal (ASN) para la detección de cambios bi-temporales, integrando pares de características asimétricas y refinamiento de características.\nAdaptive Symmetric Network (ASN) con codificador-decodificador, utilizando bloques residuales, Xception o Squeeze-and-Excitation.\nSECOND dataset (multi-clase, hasta 36 tipos de cambio). No se especifica la resolución ni los sensores.\nDetección de cambios en imágenes satelitales para análisis de urbanización, desastres naturales, etc.\nASN-ATL supera HRSCD.str4 en 1.9 en mIoU y 2.2 en SeK con bloques Xception.\nNo se especifica la resolución de las imágenes ni los sensores utilizados.\nMedia: La detección de cambios es relevante para Chile, por ejemplo, para el monitoreo de la deforestación o la expansión urbana. La aplicabilidad de ASN dependerá de la disponibilidad de datasets con características similares a SECOND.\n\n\nChen et al. (2023)\nAprovechar el conocimiento general de un modelo fundacional (SAM) para construir una red de detección de cambios eficiente (TTP).\nTime-Traveling Prompt (TTP), basado en el modelo fundacional Segment Anything Model (SAM), con ajuste fino de rango bajo, una puerta de activación de viaje en el tiempo y un decodificador multinivel.\nLEVIR-CD dataset. No se especifica la resolución ni los sensores.\nDetección de cambios en imágenes satelitales para el análisis del desarrollo urbano, monitoreo de desastres naturales, etc.\nTTP logra un F1 de 92.1 e IoU de 85.6, superando a métodos como WNet y CSTSUNet.\nDependencia del modelo fundacional SAM. No se especifica la resolución ni los sensores de las imágenes.\nMedia: El uso de modelos fundacionales como SAM para la detección de cambios en Chile es prometedor. Se requiere investigar la adaptabilidad de TTP a las condiciones locales.\n\n\nMuszynski et al. (2024)\nEvaluar la capacidad de los modelos fundacionales geoespaciales para estimar la biomasa aérea a partir de imágenes satelitales con pocos parámetros ajustables.\nDos modelos fundacionales geoespaciales (Global GFM y Local GFM) con encoder congelado y una capa de regresión.\nImágenes HLS de la NASA de tres ecorregiones en Brasil. Resolución de 30 metros.\nEstimación de biomasa aérea para el estudio del ciclo del carbono y la gestión forestal.\nLos GFMs con encoder congelado logran un rendimiento comparable al de una U-Net entrenada desde cero, con 13 veces menos parámetros.\nLa generalización a otras regiones puede verse afectada por la variabilidad en las características del bosque.\nMedia: La estimación de biomasa aérea es relevante para el monitoreo forestal en Chile. Se necesita evaluar la transferibilidad de los GFMs a los bosques nativos del país.\n\n\nTseng et al. (2023)\nAborda la escasez de datos etiquetados en la teledetección mediante el desarrollo de Presto, un modelo ligero de Transformers preentrenado para series temporales.\nTransformers con codificador-decodificador, basado en Masked Autoencoding\nDiversos: CropHarvest, TreeSatAI, EuroSAT, Sen1Floods11, S2-Agri100; resolución y tipo de datos variables (SAR, multiespectral).\nClasificación de cultivos, detección de especies de árboles, clasificación de cobertura del suelo, mapeo de inundaciones, monitoreo de cultivos.\nPrecisión variable según la tarea; en algunos casos supera a los modelos de última generación.\nLimitado a datos de series temporales de píxeles. Requiere adaptación para imágenes de alta resolución.\nAlta: útil para monitoreo agrícola, gestión forestal, y evaluación de riesgos naturales en Chile y LatAm.\n\n\nHsu and Arundel (2023)\nEvalúa el desempeño del modelo geoespacial Prithvi para el mapeo de inundaciones.\nTransformers (ViT) preentrenado con Masked Autoencoder (MAE)\nSen1Floods11; imágenes Sentinel-1 y Sentinel-2, resolución de 10m.\nMapeo de inundaciones\nTransferibilidad en datos no vistos; menor precisión que U-Net y Segformer en datos conocidos.\nRequiere 3 bandas adicionales a RGB y carece de arquitectura de extremo a extremo para análisis de alto nivel.\nAlta: relevante para evaluación de daños, planificación de emergencias y diseño de sistemas de alerta temprana en Chile.\n\n\nMall et al. (2023)\nSin información específica sobre el estudio en las fuentes.\nNo especificada.\nNo especificada.\nNo especificada.\nNo se proporcionan métricas clave.\nNo especificada.\nSin información suficiente para evaluar relevancia.\n\n\nLi et al. (2023)\nInvestiga métodos SSL para reconocimiento automático de objetivos (ATR) en imágenes SAR; propone SAR-JEPA para predicción de gradientes multiescala.\nBasado en Masked Image Modeling (MIM) y Joint-Embedding Predictive Architecture (JEPA).\nMSTAR, FUSAR-Ship, SAR-ACD; imágenes SAR.\nReconocimiento de objetivos en imágenes SAR (vehículos, barcos, aviones).\nSupera métodos SSL en clasificación con pocos disparos.\nRequiere grandes datasets SAR para preentrenamiento.\nModerada: útil para monitoreo de deforestación, pesca ilegal, y evaluación de desastres en Chile y LatAm.\n\n\nReed et al. (2023)\nExplora el aprendizaje de representaciones geoespaciales multiescala utilizando Scale-MAE, un MAE sensible a la escala.\nMAE con codificador ViT y decodificador piramidal Laplaciano\nFunctional Map of the World (FMoW), RESISC-45, entre otros; imágenes RGB y multiespectrales de diferentes resoluciones.\nClasificación de uso del suelo, segmentación semántica (detección de edificios).\nMuestra robustez a la escala y supera a SatMAE y ConvMAE en algunas tareas.\nRequiere investigación adicional sobre la influencia del tamaño del parche en el rendimiento.\nAlta: aplicable al mapeo de cobertura del suelo, monitoreo urbano y análisis de imágenes satelitales en Chile y LatAm.\n\n\n\n\n\n2.4.2 Revisión de Artículos importantes\nPara cada estudio, incluir las siguientes subsecciones:\n\n[Título del Estudio] Referencia completa: Autores, título, publicación, año.\nObjetivo del estudio: ¿Qué problema aborda el artículo?\nMétodos utilizados: Arquitecturas, datasets, técnicas de preentrenamiento.\nResultados principales: Métricas, tareas downstream, ventajas frente a otros enfoques.\nLimitaciones identificadas: Críticas o desafíos no resueltos.\nRelación con otros estudios: Comparación o integración con investigaciones similares.\n\nNota: Usa tablas o gráficos para resumir comparaciones clave, como métricas de rendimiento en tareas downstream.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Papers Review: VFM</span>"
    ]
  },
  {
    "objectID": "papers_review_vfm.html#discusión",
    "href": "papers_review_vfm.html#discusión",
    "title": "2  Papers Review: VFM",
    "section": "2.5 Discusión",
    "text": "2.5 Discusión\n\n2.5.1 Análisis comparativo\n\nComparar las fortalezas y debilidades de los estudios analizados.\nIdentificación de patrones comunes en resultados o enfoques.\n\n\n\n2.5.2 Brechas en la literatura\n\nIdentificar áreas no exploradas o que requieren mayor desarrollo.\n\n\n\n2.5.3 Implicaciones para tu investigación\n\nCómo los resultados revisados fundamentan tu propuesta de tesis.\nAdaptaciones o mejoras sugeridas para futuros trabajos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Papers Review: VFM</span>"
    ]
  },
  {
    "objectID": "papers_review_vfm.html#conclusión-y-propuesta",
    "href": "papers_review_vfm.html#conclusión-y-propuesta",
    "title": "2  Papers Review: VFM",
    "section": "2.6 Conclusión y Propuesta",
    "text": "2.6 Conclusión y Propuesta\n\n2.6.1 Conclusión de la revisión\n\nSíntesis de los hallazgos más importantes.\n\n\n\n2.6.2 Propuesta de tesis\n\nPresentar la pregunta o hipótesis principal.\nBreve descripción del enfoque metodológico y su relación con la revisión.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Papers Review: VFM</span>"
    ]
  },
  {
    "objectID": "papers_review_vfm.html#base-general",
    "href": "papers_review_vfm.html#base-general",
    "title": "2  Papers Review: VFM",
    "section": "2.7 Base General",
    "text": "2.7 Base General\n\n\n\nAbbreviation\nTitle\nPublication\nPaper\nCode & Weights\n\n\n\n\nGeoKR\nGeographical Knowledge-Driven Representation Learning for Remote Sensing Images\nTGRS2021\nGeoKR\nlink\n\n\n-\nSelf-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding\nCVPRW2021\nPaper\nlink\n\n\nGASSL\nGeography-Aware Self-Supervised Learning\nICCV2021\nGASSL\nlink\n\n\nSeCo\nSeasonal Contrast: Unsupervised Pre-Training From Uncurated Remote Sensing Data\nICCV2021\nSeCo\nlink\n\n\nDINO-MM\nSelf-supervised Vision Transformers for Joint SAR-optical Representation Learning\nIGARSS2022\nDINO-MM\nlink\n\n\nSatMAE\nSatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery\nNeurIPS2022\nSatMAE\nlink\n\n\nRS-BYOL\nSelf-Supervised Learning for Invariant Representations From Multi-Spectral and SAR Images\nJSTARS2022\nRS-BYOL\nnull\n\n\nGeCo\nGeographical Supervision Correction for Remote Sensing Representation Learning\nTGRS2022\nGeCo\nnull\n\n\nRingMo\nRingMo: A remote sensing foundation model with masked image modeling\nTGRS2022\nRingMo\nCode\n\n\nRVSA\nAdvancing plain vision transformer toward remote sensing foundation model\nTGRS2022\nRVSA\nlink\n\n\nRSP\nAn Empirical Study of Remote Sensing Pretraining\nTGRS2022\nRSP\nlink\n\n\nMATTER\nSelf-Supervised Material and Texture Representation Learning for Remote Sensing Tasks\nCVPR2022\nMATTER\nnull\n\n\nCSPT\nConsecutive Pre-Training: A Knowledge Transfer Learning Strategy with Relevant Unlabeled Data for Remote Sensing Domain\nRS2022\nCSPT\nlink\n\n\n-\nSelf-supervised Vision Transformers for Land-cover Segmentation and Classification\nCVPRW2022\nPaper\nlink\n\n\nBFM\nA billion-scale foundation model for remote sensing images\nArxiv2023\nBFM\nnull\n\n\nTOV\nTOV: The original vision model for optical remote sensing image understanding via self-supervised learning\nJSTARS2023\nTOV\nlink\n\n\nCMID\nCMID: A Unified Self-Supervised Learning Framework for Remote Sensing Image Understanding\nTGRS2023\nCMID\nlink\n\n\nRingMo-Sense\nRingMo-Sense: Remote Sensing Foundation Model for Spatiotemporal Prediction via Spatiotemporal Evolution Disentangling\nTGRS2023\nRingMo-Sense\nnull\n\n\nIaI-SimCLR\nMulti-Modal Multi-Objective Contrastive Learning for Sentinel-1/2 Imagery\nCVPRW2023\nIaI-SimCLR\nnull\n\n\nCACo\nChange-Aware Sampling and Contrastive Learning for Satellite Images\nCVPR2023\nCACo\nlink\n\n\nSatLas\nSatlasPretrain: A Large-Scale Dataset for Remote Sensing Image Understanding\nICCV2023\nSatLas\nlink\n\n\nGFM\nTowards Geospatial Foundation Models via Continual Pretraining\nICCV2023\nGFM\nlink\n\n\nScale-MAE\nScale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning\nICCV2023\nScale-MAE\nlink\n\n\nDINO-MC\nDINO-MC: Self-supervised Contrastive Learning for Remote Sensing Imagery with Multi-sized Local Crops\nArxiv2023\nDINO-MC\nlink\n\n\nCROMA\nCROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders\nNeurIPS2023\nCROMA\nlink\n\n\nCross-Scale MAE\nCross-Scale MAE: A Tale of Multiscale Exploitation in Remote Sensing\nNeurIPS2023\nCross-Scale MAE\nlink\n\n\nDeCUR\nDeCUR: decoupling common & unique representations for multimodal self-supervision\nECCV2024\nDeCUR\nlink\n\n\nPresto\nLightweight, Pre-trained Transformers for Remote Sensing Timeseries\nArxiv2023\nPresto\nlink\n\n\nCtxMIM\nCtxMIM: Context-Enhanced Masked Image Modeling for Remote Sensing Image Understanding\nArxiv2023\nCtxMIM\nnull\n\n\nFG-MAE\nFeature Guided Masked Autoencoder for Self-supervised Learning in Remote Sensing\nArxiv2023\nFG-MAE\nlink\n\n\nPrithvi\nFoundation Models for Generalist Geospatial Artificial Intelligence\nArxiv2023\nPrithvi\nlink\n\n\nRingMo-lite\nRingMo-lite: A Remote Sensing Multi-task Lightweight Network with CNN-Transformer Hybrid Framework\nArxiv2023\nRingMo-lite\nnull\n\n\n-\nA Self-Supervised Cross-Modal Remote Sensing Foundation Model with Multi-Domain Representation and Cross-Domain Fusion\nIGARSS2023\nPaper\nnull\n\n\nEarthPT\nEarthPT: a foundation model for Earth Observation\nNeurIPS2023 CCAI workshop\nEarthPT\nlink\n\n\nUSat\nUSat: A Unified Self-Supervised Encoder for Multi-Sensor Satellite Imagery\nArxiv2023\nUSat\nlink\n\n\nFoMo-Bench\nFoMo-Bench: a multi-modal, multi-scale and multi-task Forest Monitoring Benchmark for remote sensing foundation models\nArxiv2023\nFoMo-Bench\nlink\n\n\nAIEarth\nAnalytical Insight of Earth: A Cloud-Platform of Intelligent Computing for Geospatial Big Data\nArxiv2023\nAIEarth\nlink\n\n\n-\nSelf-Supervised Learning for SAR ATR with a Knowledge-Guided Predictive Architecture\nArxiv2023\nPaper\nlink\n\n\nClay\nClay Foundation Model\n-\nnull\nlink\n\n\nHydro\nHydro–A Foundation Model for Water in Satellite Imagery\n-\nnull\nlink\n\n\nU-BARN\nSelf-Supervised Spatio-Temporal Representation Learning of Satellite Image Time Series\nJSTARS2024\nPaper\nlink\n\n\nGeRSP\nGeneric Knowledge Boosted Pre-training For Remote Sensing Images\nArxiv2024\nGeRSP\nGeRSP\n\n\nSwiMDiff\nSwiMDiff: Scene-wide Matching Contrastive Learning with Diffusion Constraint for Remote Sensing Image\nArxiv2024\nSwiMDiff\nnull\n\n\nOFA-Net\nOne for All: Toward Unified Foundation Models for Earth Vision\nArxiv2024\nOFA-Net\nnull\n\n\nSMLFR\nGenerative ConvNet Foundation Model With Sparse Modeling and Low-Frequency Reconstruction for Remote Sensing Image Interpretation\nTGRS2024\nSMLFR\nlink\n\n\nSpectralGPT\nSpectralGPT: Spectral Foundation Model\nTPAMI2024\nSpectralGPT\nlink\n\n\nS2MAE\nS2MAE: A Spatial-Spectral Pretraining Foundation Model for Spectral Remote Sensing Data\nCVPR2024\nS2MAE\nnull\n\n\nSatMAE++\nRethinking Transformers Pre-training for Multi-Spectral Satellite Imagery\nCVPR2024\nSatMAE++\nlink\n\n\nmsGFM\nBridging Remote Sensors with Multisensor Geospatial Foundation Models\nCVPR2024\nmsGFM\nlink\n\n\nSkySense\nSkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal Interpretation for Earth Observation Imagery\nCVPR2024\nSkySense\nTargeted open-source\n\n\nMTP\nMTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining\nArxiv2024\nMTP\nlink\n\n\nDOFA\nNeural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities\nArxiv2024\nDOFA\nlink\n\n\nMMEarth\nMMEarth: Exploring Multi-Modal Pretext Tasks For Geospatial Representation Learning\nECCV2024\nMMEarth\nlink\n\n\nSARATR-X\nSARATR-X: A Foundation Model for Synthetic Aperture Radar Images Target Recognition\nArxiv2024\nSARATR-X\nlink\n\n\nLeMeViT\nLeMeViT: Efficient Vision Transformer with Learnable Meta Tokens for Remote Sensing Image Interpretation\nIJCAI2024\nLeMeViT\nlink\n\n\nSoftCon\nMulti-Label Guided Soft Contrastive Learning for Efficient Earth Observation Pretraining\nArxiv2024\nSoftCon\nlink\n\n\nRS-DFM\nRS-DFM: A Remote Sensing Distributed Foundation Model for Diverse Downstream Tasks\nArxiv2024\nRS-DFM\nnull\n\n\nA2-MAE\nA2-MAE: A spatial-temporal-spectral unified remote sensing pre-training method based on anchor-aware masked autoencoder\nArxiv2024\nA2-MAE\nnull\n\n\nHyperSIGMA\nHyperSIGMA: Hyperspectral Intelligence Comprehension Foundation Model\nArxiv2024\nHyperSIGMA\nlink\n\n\nSelectiveMAE\nScaling Efficient Masked Autoencoder Learning on Large Remote Sensing Dataset\nArxiv2024\nSelectiveMAE\nlink\n\n\nOmniSat\nOmniSat: Self-Supervised Modality Fusion for Earth Observation\nECCV2024\nOmniSat\nlink\n\n\nMM-VSF\nTowards a Knowledge guided Multimodal Foundation Model for Spatio-Temporal Remote Sensing Applications\nArxiv2024\nMM-VSF\nnull\n\n\nMA3E\nMasked Angle-Aware Autoencoder for Remote Sensing Images\nECCV2024\nMA3E\nlink\n\n\nSpectralEarth\nSpectralEarth: Training Hyperspectral Foundation Models at Scale\nArxiv2024\nSpectralEarth\nnull\n\n\nSenPa-MAE\nSenPa-MAE: Sensor Parameter Aware Masked Autoencoder for Multi-Satellite Self-Supervised Pretraining\nArxiv2024\nSenPa-MAE\nlink\n\n\nRingMo-Aerial\nRingMo-Aerial: An Aerial Remote Sensing Foundation Model With A Affine Transformation Contrastive Learning\nArxiv2024\nRingMo-Aerial\nnull\n\n\nSAR-JEPA\nPredicting Gradient is Better: Exploring Self-Supervised Learning for SAR ATR with a Joint-Embedding Predictive Architecture\nISPRS JPRS2024\nSAR-JEPA\nlink\n\n\nPIS\nPretrain a Remote Sensing Foundation Model by Promoting Intra-instance Similarity\nTGRS2024\nPIS\nlink\n\n\nOReole-FM\nOReole-FM: successes and challenges toward billion-parameter foundation models for high-resolution satellite imagery\nSIGSPATIAL2024\nOReole-FM\nnull\n\n\nPIEViT\nPattern Integration and Enhancement Vision Transformer for Self-supervised Learning in Remote Sensing\nArxiv2024\nPIEViT\nnull\n\n\nSatVision-TOA\nSatVision-TOA: A Geospatial Foundation Model for Coarse-Resolution All-Sky Remote Sensing Imagery\nArxiv2024\nSatVision-TOA\nlink\n\n\nRS-vHeat\nRS-vHeat: Heat Conduction Guided Efficient Remote Sensing Foundation Model\nArxiv2024\nRS-vHeat\nnull\n\n\nPrithvi-EO-2.0\nPrithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications\nArxiv2024\nPrithvi-EO-2.0\nlink\n\n\nAnySat\nAnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities\nArxiv2024\nAnySat\nlink",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Papers Review: VFM</span>"
    ]
  },
  {
    "objectID": "propuesta.html",
    "href": "propuesta.html",
    "title": "3  Propuesta",
    "section": "",
    "text": "3.1 Temática:\nVision Foundamential Models",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Propuesta</span>"
    ]
  },
  {
    "objectID": "propuesta.html#chapters",
    "href": "propuesta.html#chapters",
    "title": "3  Propuesta",
    "section": "3.2 Chapters:",
    "text": "3.2 Chapters:\n\n3.2.1 Change Detection\n\nTurberas (SAM)\nIncendios Forestales\n\n\n\n3.2.2 Land Cover Classification\n\nCapital Natural\n\n\n\n3.2.3 Segmetation\n\nEdificaciones",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Propuesta</span>"
    ]
  },
  {
    "objectID": "propuesta.html#implementación",
    "href": "propuesta.html#implementación",
    "title": "3  Propuesta",
    "section": "3.3 Implementación",
    "text": "3.3 Implementación\n\nData Cube Chile",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Propuesta</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Jakubik, Johannes, Sujit Roy, C. E. Phillips, Paolo Fraccaro, Denys\nGodwin, Bianca Zadrozny, Daniela Szwarcman, et al. 2023.\n“Foundation Models for Generalist Geospatial Artificial\nIntelligence.” https://doi.org/10.48550/arXiv.2310.18660.\n\n\nLu, Siqi, Junlin Guo, James R. Zimmer-Dauphinee, Jordan M. Nieusma, Xiao\nWang, Parker VanValkenburgh, Steven A. Wernke, and Yuankai Huo. 2024.\n“AI Foundation Models in Remote Sensing: A Survey.” https://doi.org/10.48550/arXiv.2408.03464.\n\n\nXiao, Aoran, Weihao Xuan, Junjue Wang, Jiaxing Huang, Dacheng Tao,\nShijian Lu, and Naoto Yokoya. 2024. “Foundation Models for Remote\nSensing and Earth Observation: A Survey.” https://doi.org/10.48550/arXiv.2410.16602.",
    "crumbs": [
      "References"
    ]
  }
]