<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.5">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Papers Review: VFM – Modelos Fundacionales de IA</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./propuesta.html" rel="next">
<link href="./intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ff4371ef257df69894857e99c6ad0d06.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles/style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./papers_review_vfm.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Papers Review: VFM</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Modelos Fundacionales de IA</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./papers_review_vfm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Papers Review: VFM</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./propuesta.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Propuesta</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introducción" id="toc-introducción" class="nav-link active" data-scroll-target="#introducción"><span class="header-section-number">2.1</span> Introducción</a>
  <ul class="collapse">
  <li><a href="#contexto-general" id="toc-contexto-general" class="nav-link" data-scroll-target="#contexto-general"><span class="header-section-number">2.1.1</span> Contexto general</a></li>
  <li><a href="#objetivos-de-la-revisión" id="toc-objetivos-de-la-revisión" class="nav-link" data-scroll-target="#objetivos-de-la-revisión"><span class="header-section-number">2.1.2</span> Objetivos de la revisión</a></li>
  </ul></li>
  <li><a href="#metodología" id="toc-metodología" class="nav-link" data-scroll-target="#metodología"><span class="header-section-number">2.2</span> Metodología</a>
  <ul class="collapse">
  <li><a href="#criterios-de-inclusión" id="toc-criterios-de-inclusión" class="nav-link" data-scroll-target="#criterios-de-inclusión"><span class="header-section-number">2.2.1</span> Criterios de Inclusión</a></li>
  </ul></li>
  <li><a href="#marco-conceptual" id="toc-marco-conceptual" class="nav-link" data-scroll-target="#marco-conceptual"><span class="header-section-number">2.3</span> Marco Conceptual</a>
  <ul class="collapse">
  <li><a href="#autoencoders-enmascarados-mae" id="toc-autoencoders-enmascarados-mae" class="nav-link" data-scroll-target="#autoencoders-enmascarados-mae"><span class="header-section-number">2.3.1</span> Autoencoders Enmascarados (MAE)</a></li>
  <li><a href="#aprendizaje-contrastivo" id="toc-aprendizaje-contrastivo" class="nav-link" data-scroll-target="#aprendizaje-contrastivo"><span class="header-section-number">2.3.2</span> Aprendizaje Contrastivo</a></li>
  <li><a href="#datos-multimodales-y-alineación-espacial" id="toc-datos-multimodales-y-alineación-espacial" class="nav-link" data-scroll-target="#datos-multimodales-y-alineación-espacial"><span class="header-section-number">2.3.3</span> Datos Multimodales y Alineación Espacial</a></li>
  <li><a href="#codificación-de-posición-relativa-rpe" id="toc-codificación-de-posición-relativa-rpe" class="nav-link" data-scroll-target="#codificación-de-posición-relativa-rpe"><span class="header-section-number">2.3.4</span> Codificación de Posición Relativa (RPE)</a></li>
  </ul></li>
  <li><a href="#revisión-de-los-estudios" id="toc-revisión-de-los-estudios" class="nav-link" data-scroll-target="#revisión-de-los-estudios"><span class="header-section-number">2.4</span> Revisión de los Estudios</a>
  <ul class="collapse">
  <li><a href="#tabla-resumen-general" id="toc-tabla-resumen-general" class="nav-link" data-scroll-target="#tabla-resumen-general"><span class="header-section-number">2.4.1</span> Tabla Resumen General</a></li>
  <li><a href="#revisión-de-artículos-importantes" id="toc-revisión-de-artículos-importantes" class="nav-link" data-scroll-target="#revisión-de-artículos-importantes"><span class="header-section-number">2.4.2</span> Revisión de Artículos importantes</a></li>
  </ul></li>
  <li><a href="#discusión" id="toc-discusión" class="nav-link" data-scroll-target="#discusión"><span class="header-section-number">2.5</span> Discusión</a>
  <ul class="collapse">
  <li><a href="#análisis-comparativo" id="toc-análisis-comparativo" class="nav-link" data-scroll-target="#análisis-comparativo"><span class="header-section-number">2.5.1</span> Análisis comparativo</a></li>
  <li><a href="#brechas-en-la-literatura" id="toc-brechas-en-la-literatura" class="nav-link" data-scroll-target="#brechas-en-la-literatura"><span class="header-section-number">2.5.2</span> Brechas en la literatura</a></li>
  <li><a href="#implicaciones-para-tu-investigación" id="toc-implicaciones-para-tu-investigación" class="nav-link" data-scroll-target="#implicaciones-para-tu-investigación"><span class="header-section-number">2.5.3</span> Implicaciones para tu investigación</a></li>
  </ul></li>
  <li><a href="#conclusión-y-propuesta" id="toc-conclusión-y-propuesta" class="nav-link" data-scroll-target="#conclusión-y-propuesta"><span class="header-section-number">2.6</span> Conclusión y Propuesta</a>
  <ul class="collapse">
  <li><a href="#conclusión-de-la-revisión" id="toc-conclusión-de-la-revisión" class="nav-link" data-scroll-target="#conclusión-de-la-revisión"><span class="header-section-number">2.6.1</span> Conclusión de la revisión</a></li>
  <li><a href="#propuesta-de-tesis" id="toc-propuesta-de-tesis" class="nav-link" data-scroll-target="#propuesta-de-tesis"><span class="header-section-number">2.6.2</span> Propuesta de tesis</a></li>
  </ul></li>
  <li><a href="#base-general" id="toc-base-general" class="nav-link" data-scroll-target="#base-general"><span class="header-section-number">2.7</span> Base General</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Papers Review: VFM</span></h1>
<p class="subtitle lead">Avances recientes en modelos de fundamentales de Visión en la Percepción Remota</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introducción" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introducción"><span class="header-section-number">2.1</span> Introducción</h2>
<section id="contexto-general" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="contexto-general"><span class="header-section-number">2.1.1</span> Contexto general</h3>
<p>El Remote Sensing (teledetección) ha emergido como una herramienta crucial para la observación y análisis del planeta Tierra, permitiendo monitorear fenómenos ambientales y territoriales con un nivel de detalle sin precedentes. Tecnologías como las imágenes satelitales y los sistemas aéreos han facilitado la recopilación continua de datos espaciales de alta resolución, lo que es fundamental para aplicaciones como la detección de cambios ambientales, la gestión de desastres naturales, el monitoreo de recursos y la planificación territorial.</p>
<p>En este contexto, los modelos fundacionales (Foundation Models, FMs) representan una innovación disruptiva en el campo de la inteligencia artificial aplicada a la percepción remota. Estos modelos preentrenados sobre grandes volúmenes de datos no etiquetados son capaces de aprender representaciones generales que pueden ajustarse fácilmente a tareas específicas utilizando cantidades relativamente pequeñas de datos anotados. Su flexibilidad ha permitido abordar problemas complejos en teledetección, como la segmentación semántica, la detección de objetos y la clasificación de escenas, optimizando así el análisis de datos multiespectrales y temporales.</p>
<p>Los avances recientes en Visual Foundation Models (VFMs), basados en arquitecturas avanzadas como los Vision Transformers (ViT) y enfoques autosupervisados, han potenciado su aplicabilidad a datos de alta resolución. Este progreso se alinea con la creciente capacidad de procesamiento computacional y la disponibilidad masiva de imágenes satelitales de alta calidad, las cuales aumentan continuamente en resolución espacial y temporal. Estas características posicionan a los VFMs como una herramienta prometedora para enfrentar desafíos territoriales y ambientales, particularmente en regiones como Chile y América Latina, donde los datos geoespaciales son abundantes pero subutilizados.</p>
</section>
<section id="objetivos-de-la-revisión" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="objetivos-de-la-revisión"><span class="header-section-number">2.1.2</span> Objetivos de la revisión</h3>
<p>El propósito de esta revisión bibliográfica es:</p>
<ol type="1">
<li>Explorar el estado del arte en el desarrollo y aplicaciones de los modelos fundacionales en percepción remota, con un enfoque en los Visual Foundation Models (VFMs).</li>
<li>Analizar su potencial para abordar problemas territoriales específicos en contextos geográficos de Chile y América Latina.</li>
<li>Identificar oportunidades y desafíos para la implementación de estos modelos, considerando tanto la creciente disponibilidad de datos geoespaciales como las capacidades computacionales actuales.</li>
<li>Proponer una base conceptual que sirva como fundamento para evaluar y adaptar VFMs en aplicaciones concretas, maximizando su impacto en problemas como planificación territorial, monitoreo ambiental y manejo de desastres.</li>
</ol>
</section>
</section>
<section id="metodología" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="metodología"><span class="header-section-number">2.2</span> Metodología</h2>
<section id="criterios-de-inclusión" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="criterios-de-inclusión"><span class="header-section-number">2.2.1</span> Criterios de Inclusión</h3>
<p>Para garantizar que los estudios seleccionados sean relevantes y alineados con los objetivos de esta revisión, se definirán los siguientes criterios de inclusión:</p>
<ol type="1">
<li><strong>Relevancia temática</strong></li>
</ol>
<ul>
<li>Estudios centrados en modelos fundacionales de visión (VFMs) aplicados a teledetección, específicamente en tareas como detección de cambios, clasificación de uso de suelo, segmetación semántica.</li>
<li>Investigaciones relacionadas con el uso de datos geoespaciales, imágenes satelitales o aéreas en problemas ambientales y sociales.</li>
</ul>
<ol start="2" type="1">
<li><strong>Reciente publicación</strong></li>
</ol>
<ul>
<li>Publicaciones desde 2020 en adelante, considerando la novedad del campo y su evolución rápida.</li>
<li>Publicaciones en revistas indexadas o conferencias de alto impacto en áreas de inteligencia artificial, percepción remota o teledetección.</li>
</ul>
<ol start="3" type="1">
<li><strong>Relevancia geográfica y temática</strong></li>
</ol>
<ul>
<li>Estudios que incluyan aplicaciones en problemas reales relacionados con el medio ambiente y las personas, como:</li>
<li>Cambio climático, gestión de recursos naturales, agricultura sostenible, planificación urbana, monitoreo de desastres o biodiversidad.</li>
<li>Preferencia por estudios que incluyan contextos aplicables a Chile o América Latina, aunque no se excluirán estudios con enfoques globales relevantes.</li>
</ul>
<ol start="4" type="1">
<li><strong>Disponibilidad de resultados, métricas y códigos</strong></li>
</ol>
<ul>
<li>Estudios que incluyan evaluaciones cuantitativas de desempeño, como métricas de clasificación, segmentación o detección (e.g., IoU, precisión, F1-score).</li>
<li>Descripción detallada de los datasets utilizados y la metodología aplicada, eventualmente códigos.</li>
</ul>
<ol start="5" type="1">
<li><strong>Enfoque en arquitectura y métodos de aprendizaje</strong></li>
</ol>
<ul>
<li>Modelos basados en arquitecturas avanzadas, como Vision Transformers (ViT) y enfoques autosupervisados (e.g., MAE, contrastive learning).</li>
<li>Innovaciones metodológicas para optimizar el uso de datos no etiquetados o resolver problemas multiespectrales y multitemporales.</li>
</ul>
</section>
</section>
<section id="marco-conceptual" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="marco-conceptual"><span class="header-section-number">2.3</span> Marco Conceptual</h2>
<p>A continuación, se organiza y define cada concepto clave relevante al tema de la revisión bibliográfica sobre modelos fundacionales de visión aplicados a teledetección. Estos conceptos proporcionan el fundamento teórico necesario para comprender el desarrollo y aplicación de los Visual Foundation Models (VFMs) en problemas territoriales y ambientales.</p>
<p><img src="images/VFMs_esquema.png" class="img-fluid"></p>
<p>Son modelos de aprendizaje profundo preentrenados sobre grandes volúmenes de datos no etiquetados, diseñados para tareas de visión por computadora. Su arquitectura avanzada, basada en enfoques como Vision Transformers (ViT), los hace altamente adaptables a una variedad de aplicaciones en teledetección, desde segmentación semántica hasta detección de cambios.</p>
<section id="autoencoders-enmascarados-mae" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="autoencoders-enmascarados-mae"><span class="header-section-number">2.3.1</span> Autoencoders Enmascarados (MAE)</h3>
<p>Los MAE son modelos de aprendizaje profundo diseñados para reconstruir datos de entrada a partir de una versión parcialmente enmascarada.</p>
<ul>
<li>Aplicación en VFMs: En el contexto de la teledetección, los MAE son clave para procesar imágenes satelitales, permitiendo al modelo aprender representaciones robustas incluso con datos incompletos o ruidosos.</li>
</ul>
</section>
<section id="aprendizaje-contrastivo" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="aprendizaje-contrastivo"><span class="header-section-number">2.3.2</span> Aprendizaje Contrastivo</h3>
<p>Es una técnica de aprendizaje autosupervisado que entrena al modelo para distinguir entre pares de datos similares (positivos) y disímiles (negativos).</p>
<ul>
<li>Relevancia en VFMs: Es utilizado en modelos como CROMA para aprender representaciones invariantes a sensores múltiples (e.g., radar y ópticos).</li>
</ul>
</section>
<section id="datos-multimodales-y-alineación-espacial" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="datos-multimodales-y-alineación-espacial"><span class="header-section-number">2.3.3</span> Datos Multimodales y Alineación Espacial</h3>
<ul>
<li>Datos Multimodales: Provienen de sensores diferentes (e.g., Sentinel-1 para radar y Sentinel-2 para óptico), enriqueciendo las representaciones aprendidas por los VFMs.</li>
<li>Alineación Espacial: Garantiza que los datos de distintos sensores correspondan a la misma ubicación geográfica, asegurando consistencia en las representaciones aprendidas.</li>
</ul>
</section>
<section id="codificación-de-posición-relativa-rpe" class="level3" data-number="2.3.4">
<h3 data-number="2.3.4" class="anchored" data-anchor-id="codificación-de-posición-relativa-rpe"><span class="header-section-number">2.3.4</span> Codificación de Posición Relativa (RPE)</h3>
<p>Técnica utilizada para incorporar información posicional en modelos basados en transformers. - Impacto en VFMs: Mejora la capacidad de generalización de modelos como CROMA en imágenes más grandes.</p>
</section>
</section>
<section id="revisión-de-los-estudios" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="revisión-de-los-estudios"><span class="header-section-number">2.4</span> Revisión de los Estudios</h2>
<p>Para organizar y analizar los artículos científicos se construyó una tabla que resume información importante para cumplir con los objetivo del presente capítulo, los criterios y contenidos estatal corresponde a los siguientes:</p>
<ul>
<li>Referencia: Mencionar los autores y año, incluyendo una referencia breve para seguimiento.</li>
<li>Objetivo del Estudio: Resumir el problema y la tarea específica abordada.</li>
<li>Arquitectura: Detallar el modelo utilizado (e.g., ViT, Swin Transformer, MAE).</li>
<li>Dataset: Especificar los datasets usados, incluyendo resolución y tipo (e.g., SAR, multiespectral).</li>
<li>Aplicación: Indicar cómo se aplica el modelo (e.g., monitoreo de inundaciones, detección de cultivos).</li>
<li>Métricas: Proporcionar los valores clave de desempeño.</li>
<li>Limitaciones: Identificar las barreras técnicas o de aplicación señaladas en el estudio.</li>
<li>Relevancia para Chile/LatAm: Evaluar la aplicabilidad en contextos locales según problemas ambientales o sociales.</li>
</ul>
<section id="tabla-resumen-general" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="tabla-resumen-general"><span class="header-section-number">2.4.1</span> Tabla Resumen General</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Referencia</strong></th>
<th><strong>Objetivo del Estudio</strong></th>
<th><strong>Arquitectura</strong></th>
<th><strong>Dataset</strong></th>
<th><strong>Aplicación</strong></th>
<th><strong>Métricas</strong></th>
<th><strong>Limitaciones</strong></th>
<th><strong>Relevancia para Chile/LatAm</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="citation" data-cites="fuller2023croma">Fuller, Millard, and Green (<a href="references.html#ref-fuller2023croma" role="doc-biblioref">2023</a>)</span></td>
<td>Desarrollar un framework para aprender representaciones unimodales y multimodales para la Observación de la Tierra, aprovechando objetivos autosupervisados contrastivos y de reconstrucción.</td>
<td>CROMA (autoencoder contrastivo radar-óptico enmascarado), basado en Vision Transformer (ViT), con codificación de posición 2D-ALiBi y X-ALiBi.</td>
<td>BigEarthNet, fMoW-Sentinel, EuroSAT, Canadian Cropland, DFC2020, DW-Expert, MARIDA. Usando imágenes Sentinel-2 (óptico) y Sentinel-1 (radar) a diferentes resoluciones.</td>
<td>Clasificación de imágenes satelitales y segmentación semántica.</td>
<td>Supera el estado del arte (SatMAE) en un promedio de 5.4% y 6.4% en mIoU para backbones ViT-B y ViT-L, respectivamente.</td>
<td>Requiere datos multimodales alineados espacialmente. Alto costo computacional para entrenar modelos grandes.</td>
<td>Alta: Chile posee una amplia cobertura de imágenes Sentinel, lo que permitiría aplicar CROMA para diversas aplicaciones como la clasificación de uso y cobertura del suelo, monitoreo de recursos hídricos, agricultura de precisión, entre otras.</td>
</tr>
<tr class="even">
<td><span class="citation" data-cites="szwarcman_prithvi-eo-20_2024">Szwarcman et al. (<a href="references.html#ref-szwarcman_prithvi-eo-20_2024" role="doc-biblioref">2024</a>)</span></td>
<td>El estudio busca desarrollar un modelo de base geoespacial multi-temporal llamado Prithvi-EO-2.0. El objetivo es mejorar la capacidad de generalización y la eficiencia de datos en tareas de teledetección, abordando las limitaciones de los modelos existentes en el manejo de la multi-temporalidad.</td>
<td>Prithvi-EO-2.0 utiliza una arquitectura de codificador-decodificador asimétrico basada en Vision Transformer (ViT) y entrenada con el método de autoencoder enmascarado (MAE). Se incorporan innovaciones como incrustaciones temporales y de ubicación, así como la integración de metadatos de imágenes satelitales.</td>
<td>El modelo se entrena con un conjunto de datos global de 4.2 millones de muestras de series temporales del archivo Harmonized Landsat Sentinel-2 (HLS) de la NASA a una resolución de 30m. El conjunto de datos abarca una década de imágenes satelitales.</td>
<td>Prithvi-EO-2.0 se aplica a diversas tareas de teledetección, incluyendo: - Monitoreo de desastres (detección de inundaciones, mapeo de cicatrices de incendios forestales, mapeo de intensidad de quemaduras, detección de deslizamientos de tierra). - Mapeo de uso del suelo y cultivos (segmentación de cultivos en EE. UU., clasificación multi-temporal de la cobertura del suelo y cultivos en Europa). - Monitoreo de la dinámica de los ecosistemas multimodales (estimación de la biomasa sobre el suelo, estimación de la productividad primaria bruta).</td>
<td>El rendimiento se evalúa utilizando métricas como la precisión general, el índice de Jaccard medio (mIoU), la puntuación F1 y el error cuadrático medio (RMSE) en el benchmark GEO-Bench y en las tareas posteriores. Prithvi-EO-2.0 supera a los modelos de base geoespacial existentes en el benchmark GEO-Bench hasta en un 8% y demuestra un rendimiento de vanguardia en las tareas posteriores.</td>
<td>El estudio señala la dificultad de generalizar a tareas con resoluciones espaciales significativamente diferentes a las utilizadas en el entrenamiento previo. También se menciona la necesidad de explorar estrategias de aumento de datos para mejorar el rendimiento en conjuntos de datos más pequeños.</td>
<td>Prithvi-EO-2.0 tiene un alto potencial para abordar problemas ambientales y sociales relevantes en Chile y Latinoamérica. Su capacidad para el monitoreo de desastres, el mapeo de la cobertura del suelo y la estimación de variables ecosistémicas podría ser valiosa para: - Monitoreo de incendios forestales y sequías. - Gestión de recursos hídricos. - Monitoreo de la deforestación y cambios en el uso del suelo. - Evaluación de la productividad agrícola y seguridad alimentaria. - Evaluación de riesgos de desastres naturales como deslizamientos de tierra e inundaciones.</td>
</tr>
<tr class="odd">
<td><span class="citation" data-cites="astruc2024anysat">Astruc et al. (<a href="references.html#ref-astruc2024anysat" role="doc-biblioref">2024</a>)</span></td>
<td>El estudio busca desarrollar un modelo de aprendizaje automático (AnySat) capaz de analizar datos de Observación de la Tierra (EO) con diferentes resoluciones, escalas y modalidades. Busca superar las limitaciones de los modelos existentes que, generalmente, se entrenan en un solo conjunto de datos con un formato específico. El objetivo es crear un modelo base que se pueda aplicar a diversos conjuntos de datos de EO sin necesidad de reentrenamiento desde cero.</td>
<td>AnySat utiliza una arquitectura de codificador-combinador-predictor basada en Vision Transformer (ViT) y una estrategia de entrenamiento auto-supervisado basada en la arquitectura de predicción de incrustaciones conjuntas (JEPA). Se añaden codificadores de parches adaptables a la escala para manejar diferentes tamaños de entrada y un codificador temporal ligero para datos de series de tiempo.</td>
<td>GeoPlex es un conjunto de datos multimodal con 11 modalidades, integrando imágenes aéreas, satelitales, radares y sensores ópticos. Incluye diversas resoluciones, tiempos de revisita y canales, complementado con 4 conjuntos externos (SICKLE, BraDD-S1TS, TimeSen2Crop, Sen1Flood11).</td>
<td>AnySat se evalúa en diversas tareas de monitoreo ambiental, incluyendo: - Mapeo de la cobertura terrestre. - Identificación de especies de árboles. - Clasificación del tipo de cultivo. - Detección de cambios (deforestación). - Segmentación de inundaciones.</td>
<td>Se utilizan métricas como la puntuación F1, el índice de Jaccard medio (mIoU), la precisión general (OA) para evaluar el rendimiento de AnySat en las diferentes tareas. AnySat logra resultados de vanguardia en las tareas de clasificación, segmentación y detección de cambios en diversos conjuntos de datos, incluso superando a modelos especializados entrenados en conjuntos de datos específicos.</td>
<td>Se menciona la dificultad de generalizar a tareas con resoluciones espaciales significativamente diferentes a las utilizadas en el entrenamiento previo. Aunque se ha demostrado que el modelo maneja diferentes resoluciones, se necesita más investigación para garantizar un rendimiento óptimo en todas las escalas.</td>
<td>AnySat tiene gran potencial en Chile y Latinoamérica para monitorear deforestación, desastres naturales, agricultura, y recursos hídricos. Su capacidad multiescalar y multimodal es clave, aunque requiere adaptación a contextos locales y diferentes resoluciones espaciales.</td>
</tr>
<tr class="even">
<td>Wang et al.&nbsp;(2022)</td>
<td>Proponer una red neuronal (ASN) para la detección de cambios bi-temporales, integrando pares de características asimétricas y refinamiento de características.</td>
<td>Adaptive Symmetric Network (ASN) con codificador-decodificador, utilizando bloques residuales, Xception o Squeeze-and-Excitation.</td>
<td>SECOND dataset (multi-clase, hasta 36 tipos de cambio). No se especifica la resolución ni los sensores.</td>
<td>Detección de cambios en imágenes satelitales para análisis de urbanización, desastres naturales, etc.</td>
<td>ASN-ATL supera HRSCD.str4 en 1.9 en mIoU y 2.2 en SeK con bloques Xception.</td>
<td>No se especifica la resolución de las imágenes ni los sensores utilizados.</td>
<td>Media: La detección de cambios es relevante para Chile, por ejemplo, para el monitoreo de la deforestación o la expansión urbana. La aplicabilidad de ASN dependerá de la disponibilidad de datasets con características similares a SECOND.</td>
</tr>
<tr class="odd">
<td>Chen et al.&nbsp;(2023)</td>
<td>Aprovechar el conocimiento general de un modelo fundacional (SAM) para construir una red de detección de cambios eficiente (TTP).</td>
<td>Time-Traveling Prompt (TTP), basado en el modelo fundacional Segment Anything Model (SAM), con ajuste fino de rango bajo, una puerta de activación de viaje en el tiempo y un decodificador multinivel.</td>
<td>LEVIR-CD dataset. No se especifica la resolución ni los sensores.</td>
<td>Detección de cambios en imágenes satelitales para el análisis del desarrollo urbano, monitoreo de desastres naturales, etc.</td>
<td>TTP logra un F1 de 92.1 e IoU de 85.6, superando a métodos como WNet y CSTSUNet.</td>
<td>Dependencia del modelo fundacional SAM. No se especifica la resolución ni los sensores de las imágenes.</td>
<td>Media: El uso de modelos fundacionales como SAM para la detección de cambios en Chile es prometedor. Se requiere investigar la adaptabilidad de TTP a las condiciones locales.</td>
</tr>
<tr class="even">
<td>Muszynski et al.&nbsp;(2024)</td>
<td>Evaluar la capacidad de los modelos fundacionales geoespaciales para estimar la biomasa aérea a partir de imágenes satelitales con pocos parámetros ajustables.</td>
<td>Dos modelos fundacionales geoespaciales (Global GFM y Local GFM) con encoder congelado y una capa de regresión.</td>
<td>Imágenes HLS de la NASA de tres ecorregiones en Brasil. Resolución de 30 metros.</td>
<td>Estimación de biomasa aérea para el estudio del ciclo del carbono y la gestión forestal.</td>
<td>Los GFMs con encoder congelado logran un rendimiento comparable al de una U-Net entrenada desde cero, con 13 veces menos parámetros.</td>
<td>La generalización a otras regiones puede verse afectada por la variabilidad en las características del bosque.</td>
<td>Media: La estimación de biomasa aérea es relevante para el monitoreo forestal en Chile. Se necesita evaluar la transferibilidad de los GFMs a los bosques nativos del país.</td>
</tr>
<tr class="odd">
<td>Tseng et al.&nbsp;(2023)</td>
<td>Aborda la escasez de datos etiquetados en la teledetección mediante el desarrollo de Presto, un modelo ligero de Transformers preentrenado para series temporales.</td>
<td>PRESTO , Transformers con codificador-decodificador, basado en Masked Autoencoding</td>
<td>Diversos: CropHarvest, TreeSatAI, EuroSAT, Sen1Floods11, S2-Agri100; resolución y tipo de datos variables (SAR, multiespectral).</td>
<td>Clasificación de cultivos, detección de especies de árboles, clasificación de cobertura del suelo, mapeo de inundaciones, monitoreo de cultivos.</td>
<td>Precisión variable según la tarea; en algunos casos supera a los modelos de última generación.</td>
<td>Limitado a datos de series temporales de píxeles. Requiere adaptación para imágenes de alta resolución.</td>
<td>Alta: útil para monitoreo agrícola, gestión forestal, y evaluación de riesgos naturales en Chile y LatAm.</td>
</tr>
<tr class="even">
<td>Hsu and Arundel (2023)</td>
<td>Evalúa el desempeño del modelo geoespacial Prithvi para el mapeo de inundaciones.</td>
<td>Transformers (ViT) preentrenado con Masked Autoencoder (MAE)</td>
<td>Sen1Floods11; imágenes Sentinel-1 y Sentinel-2, resolución de 10m.</td>
<td>Mapeo de inundaciones</td>
<td>Transferibilidad en datos no vistos; menor precisión que U-Net y Segformer en datos conocidos.</td>
<td>Requiere 3 bandas adicionales a RGB y carece de arquitectura de extremo a extremo para análisis de alto nivel.</td>
<td>Alta: relevante para evaluación de daños, planificación de emergencias y diseño de sistemas de alerta temprana en Chile.</td>
</tr>
<tr class="odd">
<td>Li et al.&nbsp;(2023)</td>
<td>Investiga métodos SSL para reconocimiento automático de objetivos (ATR) en imágenes SAR; propone SAR-JEPA para predicción de gradientes multiescala.</td>
<td>Basado en Masked Image Modeling (MIM) y Joint-Embedding Predictive Architecture (JEPA).</td>
<td>MSTAR, FUSAR-Ship, SAR-ACD; imágenes SAR.</td>
<td>Reconocimiento de objetivos en imágenes SAR (vehículos, barcos, aviones).</td>
<td>Supera métodos SSL en clasificación con pocos disparos.</td>
<td>Requiere grandes datasets SAR para preentrenamiento.</td>
<td>Moderada: útil para monitoreo de deforestación, pesca ilegal, y evaluación de desastres en Chile y LatAm.</td>
</tr>
<tr class="even">
<td>Reed et al.&nbsp;(2023)</td>
<td>Explora el aprendizaje de representaciones geoespaciales multiescala utilizando Scale-MAE, un MAE sensible a la escala.</td>
<td>MAE con codificador ViT y decodificador piramidal Laplaciano</td>
<td>Functional Map of the World (FMoW), RESISC-45, entre otros; imágenes RGB y multiespectrales de diferentes resoluciones.</td>
<td>Clasificación de uso del suelo, segmentación semántica (detección de edificios).</td>
<td>Muestra robustez a la escala y supera a SatMAE y ConvMAE en algunas tareas.</td>
<td>Requiere investigación adicional sobre la influencia del tamaño del parche en el rendimiento.</td>
<td>Alta: aplicable al mapeo de cobertura del suelo, monitoreo urbano y análisis de imágenes satelitales en Chile y LatAm.</td>
</tr>
</tbody>
</table>
</section>
<section id="revisión-de-artículos-importantes" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="revisión-de-artículos-importantes"><span class="header-section-number">2.4.2</span> Revisión de Artículos importantes</h3>
<p>Para cada estudio, incluir las siguientes subsecciones:</p>
<ul>
<li>[Título del Estudio] Referencia completa: Autores, título, publicación, año.</li>
<li>Objetivo del estudio: ¿Qué problema aborda el artículo?</li>
<li>Métodos utilizados: Arquitecturas, datasets, técnicas de preentrenamiento.</li>
<li>Resultados principales: Métricas, tareas downstream, ventajas frente a otros enfoques.</li>
<li>Limitaciones identificadas: Críticas o desafíos no resueltos.</li>
<li>Relación con otros estudios: Comparación o integración con investigaciones similares.</li>
</ul>
<p><em>Nota: Usa tablas o gráficos para resumir comparaciones clave, como métricas de rendimiento en tareas downstream.</em></p>
</section>
</section>
<section id="discusión" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="discusión"><span class="header-section-number">2.5</span> Discusión</h2>
<section id="análisis-comparativo" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="análisis-comparativo"><span class="header-section-number">2.5.1</span> Análisis comparativo</h3>
<ul>
<li>Comparar las fortalezas y debilidades de los estudios analizados.</li>
<li>Identificación de patrones comunes en resultados o enfoques.</li>
</ul>
</section>
<section id="brechas-en-la-literatura" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="brechas-en-la-literatura"><span class="header-section-number">2.5.2</span> Brechas en la literatura</h3>
<ul>
<li>Identificar áreas no exploradas o que requieren mayor desarrollo.</li>
</ul>
</section>
<section id="implicaciones-para-tu-investigación" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="implicaciones-para-tu-investigación"><span class="header-section-number">2.5.3</span> Implicaciones para tu investigación</h3>
<ul>
<li>Cómo los resultados revisados fundamentan tu propuesta de tesis.</li>
<li>Adaptaciones o mejoras sugeridas para futuros trabajos.</li>
</ul>
</section>
</section>
<section id="conclusión-y-propuesta" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="conclusión-y-propuesta"><span class="header-section-number">2.6</span> Conclusión y Propuesta</h2>
<section id="conclusión-de-la-revisión" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="conclusión-de-la-revisión"><span class="header-section-number">2.6.1</span> Conclusión de la revisión</h3>
<ul>
<li>Síntesis de los hallazgos más importantes.</li>
</ul>
</section>
<section id="propuesta-de-tesis" class="level3" data-number="2.6.2">
<h3 data-number="2.6.2" class="anchored" data-anchor-id="propuesta-de-tesis"><span class="header-section-number">2.6.2</span> Propuesta de tesis</h3>
<ul>
<li>Presentar la pregunta o hipótesis principal.</li>
<li>Breve descripción del enfoque metodológico y su relación con la revisión.</li>
</ul>
</section>
</section>
<section id="base-general" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="base-general"><span class="header-section-number">2.7</span> Base General</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Abbreviation</th>
<th>Title</th>
<th style="text-align: center;">Publication</th>
<th style="text-align: center;">Paper</th>
<th style="text-align: center;">Code &amp; Weights</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>GeoKR</strong></td>
<td><strong>Geographical Knowledge-Driven Representation Learning for Remote Sensing Images</strong></td>
<td style="text-align: center;">TGRS2021</td>
<td style="text-align: center;"><a href="https://ieeexplore.ieee.org/abstract/document/9559903">GeoKR</a></td>
<td style="text-align: center;"><a href="https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>-</strong></td>
<td><strong>Self-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding</strong></td>
<td style="text-align: center;">CVPRW2021</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Stojnic_Self-Supervised_Learning_of_Remote_Sensing_Scene_Representations_Using_Contrastive_Multiview_CVPRW_2021_paper.html">Paper</a></td>
<td style="text-align: center;"><a href="https://github.com/vladan-stojnic/CMC-RSSR">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>GASSL</strong></td>
<td><strong>Geography-Aware Self-Supervised Learning</strong></td>
<td style="text-align: center;">ICCV2021</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2021/html/Ayush_Geography-Aware_Self-Supervised_Learning_ICCV_2021_paper.html">GASSL</a></td>
<td style="text-align: center;"><a href="https://github.com/sustainlab-group/geography-aware-ssl">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>SeCo</strong></td>
<td><strong>Seasonal Contrast: Unsupervised Pre-Training From Uncurated Remote Sensing Data</strong></td>
<td style="text-align: center;">ICCV2021</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2021/html/Manas_Seasonal_Contrast_Unsupervised_Pre-Training_From_Uncurated_Remote_Sensing_Data_ICCV_2021_paper.html">SeCo</a></td>
<td style="text-align: center;"><a href="https://github.com/ServiceNow/seasonal-contrast">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>DINO-MM</strong></td>
<td><strong>Self-supervised Vision Transformers for Joint SAR-optical Representation Learning</strong></td>
<td style="text-align: center;">IGARSS2022</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2204.05381">DINO-MM</a></td>
<td style="text-align: center;"><a href="https://github.com/zhu-xlab/DINO-MM">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>SatMAE</strong></td>
<td><strong>SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery</strong></td>
<td style="text-align: center;">NeurIPS2022</td>
<td style="text-align: center;"><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/01c561df365429f33fcd7a7faa44c985-Abstract-Conference.html">SatMAE</a></td>
<td style="text-align: center;"><a href="https://github.com/sustainlab-group/SatMAE">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>RS-BYOL</strong></td>
<td><strong>Self-Supervised Learning for Invariant Representations From Multi-Spectral and SAR Images</strong></td>
<td style="text-align: center;">JSTARS2022</td>
<td style="text-align: center;"><a href="https://ieeexplore.ieee.org/abstract/document/9880533">RS-BYOL</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>GeCo</strong></td>
<td><strong>Geographical Supervision Correction for Remote Sensing Representation Learning</strong></td>
<td style="text-align: center;">TGRS2022</td>
<td style="text-align: center;"><a href="https://ieeexplore.ieee.org/abstract/document/9869651">GeCo</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>RingMo</strong></td>
<td><strong>RingMo: A remote sensing foundation model with masked image modeling</strong></td>
<td style="text-align: center;">TGRS2022</td>
<td style="text-align: center;"><a href="https://ieeexplore.ieee.org/abstract/document/9844015">RingMo</a></td>
<td style="text-align: center;"><a href="https://github.com/comeony/RingMo">Code</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>RVSA</strong></td>
<td><strong>Advancing plain vision transformer toward remote sensing foundation model</strong></td>
<td style="text-align: center;">TGRS2022</td>
<td style="text-align: center;"><a href="https://ieeexplore.ieee.org/abstract/document/9956816">RVSA</a></td>
<td style="text-align: center;"><a href="https://github.com/ViTAE-Transformer/Remote-Sensing-RVSA">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>RSP</strong></td>
<td><strong>An Empirical Study of Remote Sensing Pretraining</strong></td>
<td style="text-align: center;">TGRS2022</td>
<td style="text-align: center;"><a href="https://ieeexplore.ieee.org/abstract/document/9782149">RSP</a></td>
<td style="text-align: center;"><a href="https://github.com/ViTAE-Transformer/Remote-Sensing-RVSA">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>MATTER</strong></td>
<td><strong>Self-Supervised Material and Texture Representation Learning for Remote Sensing Tasks</strong></td>
<td style="text-align: center;">CVPR2022</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Akiva_Self-Supervised_Material_and_Texture_Representation_Learning_for_Remote_Sensing_Tasks_CVPR_2022_paper.html">MATTER</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>CSPT</strong></td>
<td><strong>Consecutive Pre-Training: A Knowledge Transfer Learning Strategy with Relevant Unlabeled Data for Remote Sensing Domain</strong></td>
<td style="text-align: center;">RS2022</td>
<td style="text-align: center;"><a href="https://www.mdpi.com/2072-4292/14/22/5675#">CSPT</a></td>
<td style="text-align: center;"><a href="https://github.com/ZhAnGToNG1/transfer_learning_cspt">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>-</strong></td>
<td><strong>Self-supervised Vision Transformers for Land-cover Segmentation and Classification</strong></td>
<td style="text-align: center;">CVPRW2022</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022W/EarthVision/html/Scheibenreif_Self-Supervised_Vision_Transformers_for_Land-Cover_Segmentation_and_Classification_CVPRW_2022_paper.html">Paper</a></td>
<td style="text-align: center;"><a href="https://github.com/HSG-AIML/SSLTransformerRS">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>BFM</strong></td>
<td><strong>A billion-scale foundation model for remote sensing images</strong></td>
<td style="text-align: center;">Arxiv2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2304.05215">BFM</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>TOV</strong></td>
<td><strong>TOV: The original vision model for optical remote sensing image understanding via self-supervised learning</strong></td>
<td style="text-align: center;">JSTARS2023</td>
<td style="text-align: center;"><a href="https://ieeexplore.ieee.org/abstract/document/10110958">TOV</a></td>
<td style="text-align: center;"><a href="https://github.com/GeoX-Lab/G-RSIM/tree/main/TOV_v1">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>CMID</strong></td>
<td><strong>CMID: A Unified Self-Supervised Learning Framework for Remote Sensing Image Understanding</strong></td>
<td style="text-align: center;">TGRS2023</td>
<td style="text-align: center;"><a href="https://ieeexplore.ieee.org/abstract/document/10105625">CMID</a></td>
<td style="text-align: center;"><a href="https://github.com/NJU-LHRS/official-CMID">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>RingMo-Sense</strong></td>
<td><strong>RingMo-Sense: Remote Sensing Foundation Model for Spatiotemporal Prediction via Spatiotemporal Evolution Disentangling</strong></td>
<td style="text-align: center;">TGRS2023</td>
<td style="text-align: center;"><a href="https://ieeexplore.ieee.org/abstract/document/10254320">RingMo-Sense</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>IaI-SimCLR</strong></td>
<td><strong>Multi-Modal Multi-Objective Contrastive Learning for Sentinel-1/2 Imagery</strong></td>
<td style="text-align: center;">CVPRW2023</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Prexl_Multi-Modal_Multi-Objective_Contrastive_Learning_for_Sentinel-12_Imagery_CVPRW_2023_paper.html">IaI-SimCLR</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>CACo</strong></td>
<td><strong>Change-Aware Sampling and Contrastive Learning for Satellite Images</strong></td>
<td style="text-align: center;">CVPR2023</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mall_Change-Aware_Sampling_and_Contrastive_Learning_for_Satellite_Images_CVPR_2023_paper.html">CACo</a></td>
<td style="text-align: center;"><a href="https://github.com/utkarshmall13/CACo">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>SatLas</strong></td>
<td><strong>SatlasPretrain: A Large-Scale Dataset for Remote Sensing Image Understanding</strong></td>
<td style="text-align: center;">ICCV2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2211.15660">SatLas</a></td>
<td style="text-align: center;"><a href="https://github.com/allenai/satlas">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>GFM</strong></td>
<td><strong>Towards Geospatial Foundation Models via Continual Pretraining</strong></td>
<td style="text-align: center;">ICCV2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2302.04476">GFM</a></td>
<td style="text-align: center;"><a href="https://github.com/mmendiet/GFM">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Scale-MAE</strong></td>
<td><strong>Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning</strong></td>
<td style="text-align: center;">ICCV2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2212.14532">Scale-MAE</a></td>
<td style="text-align: center;"><a href="https://github.com/bair-climate-initiative/scale-mae">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>DINO-MC</strong></td>
<td><strong>DINO-MC: Self-supervised Contrastive Learning for Remote Sensing Imagery with Multi-sized Local Crops</strong></td>
<td style="text-align: center;">Arxiv2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2303.06670">DINO-MC</a></td>
<td style="text-align: center;"><a href="https://github.com/WennyXY/DINO-MC">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>CROMA</strong></td>
<td><strong>CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders</strong></td>
<td style="text-align: center;">NeurIPS2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2311.00566.pdf">CROMA</a></td>
<td style="text-align: center;"><a href="https://github.com/antofuller/CROMA">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Cross-Scale MAE</strong></td>
<td><strong>Cross-Scale MAE: A Tale of Multiscale Exploitation in Remote Sensing</strong></td>
<td style="text-align: center;">NeurIPS2023</td>
<td style="text-align: center;"><a href="https://openreview.net/pdf?id=5oEVdOd6TV">Cross-Scale MAE</a></td>
<td style="text-align: center;"><a href="https://github.com/aicip/Cross-Scale-MAE">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>DeCUR</strong></td>
<td><strong>DeCUR: decoupling common &amp; unique representations for multimodal self-supervision</strong></td>
<td style="text-align: center;">ECCV2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2309.05300">DeCUR</a></td>
<td style="text-align: center;"><a href="https://github.com/zhu-xlab/DeCUR">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Presto</strong></td>
<td><strong>Lightweight, Pre-trained Transformers for Remote Sensing Timeseries</strong></td>
<td style="text-align: center;">Arxiv2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2304.14065">Presto</a></td>
<td style="text-align: center;"><a href="https://github.com/nasaharvest/presto">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>CtxMIM</strong></td>
<td><strong>CtxMIM: Context-Enhanced Masked Image Modeling for Remote Sensing Image Understanding</strong></td>
<td style="text-align: center;">Arxiv2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2310.00022">CtxMIM</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>FG-MAE</strong></td>
<td><strong>Feature Guided Masked Autoencoder for Self-supervised Learning in Remote Sensing</strong></td>
<td style="text-align: center;">Arxiv2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2310.18653">FG-MAE</a></td>
<td style="text-align: center;"><a href="https://github.com/zhu-xlab/FGMAE">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Prithvi</strong></td>
<td><strong>Foundation Models for Generalist Geospatial Artificial Intelligence</strong></td>
<td style="text-align: center;">Arxiv2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2310.18660">Prithvi</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/ibm-nasa-geospatial">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>RingMo-lite</strong></td>
<td><strong>RingMo-lite: A Remote Sensing Multi-task Lightweight Network with CNN-Transformer Hybrid Framework</strong></td>
<td style="text-align: center;">Arxiv2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2309.09003">RingMo-lite</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>-</strong></td>
<td><strong>A Self-Supervised Cross-Modal Remote Sensing Foundation Model with Multi-Domain Representation and Cross-Domain Fusion</strong></td>
<td style="text-align: center;">IGARSS2023</td>
<td style="text-align: center;"><a href="https://ieeexplore.ieee.org/abstract/document/10282433">Paper</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>EarthPT</strong></td>
<td><strong>EarthPT: a foundation model for Earth Observation</strong></td>
<td style="text-align: center;">NeurIPS2023 CCAI workshop</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2309.07207">EarthPT</a></td>
<td style="text-align: center;"><a href="https://github.com/aspiaspace/EarthPT">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>USat</strong></td>
<td><strong>USat: A Unified Self-Supervised Encoder for Multi-Sensor Satellite Imagery</strong></td>
<td style="text-align: center;">Arxiv2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2312.02199">USat</a></td>
<td style="text-align: center;"><a href="https://github.com/stanfordmlgroup/USat">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>FoMo-Bench</strong></td>
<td><strong>FoMo-Bench: a multi-modal, multi-scale and multi-task Forest Monitoring Benchmark for remote sensing foundation models</strong></td>
<td style="text-align: center;">Arxiv2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2312.10114">FoMo-Bench</a></td>
<td style="text-align: center;"><a href="https://github.com/RolnickLab/FoMo-Bench">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>AIEarth</strong></td>
<td><strong>Analytical Insight of Earth: A Cloud-Platform of Intelligent Computing for Geospatial Big Data</strong></td>
<td style="text-align: center;">Arxiv2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2312.16385">AIEarth</a></td>
<td style="text-align: center;"><a href="https://engine-aiearth.aliyun.com/#/">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>-</strong></td>
<td><strong>Self-Supervised Learning for SAR ATR with a Knowledge-Guided Predictive Architecture</strong></td>
<td style="text-align: center;">Arxiv2023</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2311.15153v4">Paper</a></td>
<td style="text-align: center;"><a href="https://github.com/waterdisappear/SAR-JEPA">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Clay</strong></td>
<td><strong>Clay Foundation Model</strong></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;"><a href="https://clay-foundation.github.io/model/">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Hydro</strong></td>
<td><strong>Hydro–A Foundation Model for Water in Satellite Imagery</strong></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;"><a href="https://github.com/isaaccorley/hydro-foundation-model">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>U-BARN</strong></td>
<td><strong>Self-Supervised Spatio-Temporal Representation Learning of Satellite Image Time Series</strong></td>
<td style="text-align: center;">JSTARS2024</td>
<td style="text-align: center;"><a href="https://ieeexplore.ieee.org/document/10414422">Paper</a></td>
<td style="text-align: center;"><a href="https://src.koda.cnrs.fr/iris.dumeur/ssl_ubarn">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>GeRSP</strong></td>
<td><strong>Generic Knowledge Boosted Pre-training For Remote Sensing Images</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2401.04614">GeRSP</a></td>
<td style="text-align: center;"><a href="https://github.com/floatingstarZ/GeRSP">GeRSP</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>SwiMDiff</strong></td>
<td><strong>SwiMDiff: Scene-wide Matching Contrastive Learning with Diffusion Constraint for Remote Sensing Image</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2401.05093">SwiMDiff</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>OFA-Net</strong></td>
<td><strong>One for All: Toward Unified Foundation Models for Earth Vision</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2401.07527">OFA-Net</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>SMLFR</strong></td>
<td><strong>Generative ConvNet Foundation Model With Sparse Modeling and Low-Frequency Reconstruction for Remote Sensing Image Interpretation</strong></td>
<td style="text-align: center;">TGRS2024</td>
<td style="text-align: center;"><a href="https://ieeexplore.ieee.org/abstract/document/10378718">SMLFR</a></td>
<td style="text-align: center;"><a href="https://github.com/HIT-SIRS/SMLFR">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>SpectralGPT</strong></td>
<td><strong>SpectralGPT: Spectral Foundation Model</strong></td>
<td style="text-align: center;">TPAMI2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2311.07113">SpectralGPT</a></td>
<td style="text-align: center;"><a href="https://github.com/danfenghong/IEEE_TPAMI_SpectralGPT">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>S2MAE</strong></td>
<td><strong>S2MAE: A Spatial-Spectral Pretraining Foundation Model for Spectral Remote Sensing Data</strong></td>
<td style="text-align: center;">CVPR2024</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_S2MAE_A_Spatial-Spectral_Pretraining_Foundation_Model_for_Spectral_Remote_Sensing_CVPR_2024_paper.pdf">S2MAE</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>SatMAE++</strong></td>
<td><strong>Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery</strong></td>
<td style="text-align: center;">CVPR2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2403.05419">SatMAE++</a></td>
<td style="text-align: center;"><a href="https://github.com/techmn/satmae_pp">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>msGFM</strong></td>
<td><strong>Bridging Remote Sensors with Multisensor Geospatial Foundation Models</strong></td>
<td style="text-align: center;">CVPR2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2404.01260">msGFM</a></td>
<td style="text-align: center;"><a href="https://github.com/boranhan/Geospatial_Foundation_Models">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>SkySense</strong></td>
<td><strong>SkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal Interpretation for Earth Observation Imagery</strong></td>
<td style="text-align: center;">CVPR2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2312.10115">SkySense</a></td>
<td style="text-align: center;">Targeted open-source</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>MTP</strong></td>
<td><strong>MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2403.13430">MTP</a></td>
<td style="text-align: center;"><a href="https://github.com/ViTAE-Transformer/MTP">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>DOFA</strong></td>
<td><strong>Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2403.15356">DOFA</a></td>
<td style="text-align: center;"><a href="https://github.com/zhu-xlab/DOFA">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>MMEarth</strong></td>
<td><strong>MMEarth: Exploring Multi-Modal Pretext Tasks For Geospatial Representation Learning</strong></td>
<td style="text-align: center;">ECCV2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2405.02771">MMEarth</a></td>
<td style="text-align: center;"><a href="https://vishalned.github.io/mmearth/">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>SARATR-X</strong></td>
<td><strong>SARATR-X: A Foundation Model for Synthetic Aperture Radar Images Target Recognition</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2405.09365">SARATR-X</a></td>
<td style="text-align: center;"><a href="https://github.com/waterdisappear/SARATR-X">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>LeMeViT</strong></td>
<td><strong>LeMeViT: Efficient Vision Transformer with Learnable Meta Tokens for Remote Sensing Image Interpretation</strong></td>
<td style="text-align: center;">IJCAI2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2405.09789">LeMeViT</a></td>
<td style="text-align: center;"><a href="https://github.com/ViTAE-Transformer/LeMeViT/tree/main?tab=readme-ov-file">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>SoftCon</strong></td>
<td><strong>Multi-Label Guided Soft Contrastive Learning for Efficient Earth Observation Pretraining</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2405.20462">SoftCon</a></td>
<td style="text-align: center;"><a href="https://github.com/zhu-xlab/softcon?tab=readme-ov-file">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>RS-DFM</strong></td>
<td><strong>RS-DFM: A Remote Sensing Distributed Foundation Model for Diverse Downstream Tasks</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2406.07032">RS-DFM</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>A2-MAE</strong></td>
<td><strong>A2-MAE: A spatial-temporal-spectral unified remote sensing pre-training method based on anchor-aware masked autoencoder</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2406.08079">A2-MAE</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>HyperSIGMA</strong></td>
<td><strong>HyperSIGMA: Hyperspectral Intelligence Comprehension Foundation Model</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2406.11519">HyperSIGMA</a></td>
<td style="text-align: center;"><a href="https://github.com/WHU-Sigma/HyperSIGMA?tab=readme-ov-file">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>SelectiveMAE</strong></td>
<td><strong>Scaling Efficient Masked Autoencoder Learning on Large Remote Sensing Dataset</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2406.11933">SelectiveMAE</a></td>
<td style="text-align: center;"><a href="https://github.com/Fengxiang23/SelectiveMAE">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>OmniSat</strong></td>
<td><strong>OmniSat: Self-Supervised Modality Fusion for Earth Observation</strong></td>
<td style="text-align: center;">ECCV2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2404.08351">OmniSat</a></td>
<td style="text-align: center;"><a href="https://github.com/gastruc/OmniSat?tab=readme-ov-file">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>MM-VSF</strong></td>
<td><strong>Towards a Knowledge guided Multimodal Foundation Model for Spatio-Temporal Remote Sensing Applications</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2407.19660">MM-VSF</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>MA3E</strong></td>
<td><strong>Masked Angle-Aware Autoencoder for Remote Sensing Images</strong></td>
<td style="text-align: center;">ECCV2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2408.01946">MA3E</a></td>
<td style="text-align: center;"><a href="https://github.com/benesakitam/MA3E">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>SpectralEarth</strong></td>
<td><strong>SpectralEarth: Training Hyperspectral Foundation Models at Scale</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2408.08447">SpectralEarth</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>SenPa-MAE</strong></td>
<td><strong>SenPa-MAE: Sensor Parameter Aware Masked Autoencoder for Multi-Satellite Self-Supervised Pretraining</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2408.11000">SenPa-MAE</a></td>
<td style="text-align: center;"><a href="https://github.com/JonathanPrexl/SenPa-MAE">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>RingMo-Aerial</strong></td>
<td><strong>RingMo-Aerial: An Aerial Remote Sensing Foundation Model With A Affine Transformation Contrastive Learning</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2409.13366">RingMo-Aerial</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>SAR-JEPA</strong></td>
<td><strong>Predicting Gradient is Better: Exploring Self-Supervised Learning for SAR ATR with a Joint-Embedding Predictive Architecture</strong></td>
<td style="text-align: center;">ISPRS JPRS2024</td>
<td style="text-align: center;"><a href="https://www.sciencedirect.com/science/article/pii/S0924271624003514">SAR-JEPA</a></td>
<td style="text-align: center;"><a href="https://github.com/waterdisappear/SAR-JEPA">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>PIS</strong></td>
<td><strong>Pretrain a Remote Sensing Foundation Model by Promoting Intra-instance Similarity</strong></td>
<td style="text-align: center;">TGRS2024</td>
<td style="text-align: center;"><a href="https://ieeexplore.ieee.org/abstract/document/10697182">PIS</a></td>
<td style="text-align: center;"><a href="https://github.com/ShawnAn-WHU/PIS">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>OReole-FM</strong></td>
<td><strong>OReole-FM: successes and challenges toward billion-parameter foundation models for high-resolution satellite imagery</strong></td>
<td style="text-align: center;">SIGSPATIAL2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2410.19965">OReole-FM</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>PIEViT</strong></td>
<td><strong>Pattern Integration and Enhancement Vision Transformer for Self-supervised Learning in Remote Sensing</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/pdf/2411.06091">PIEViT</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>SatVision-TOA</strong></td>
<td><strong>SatVision-TOA: A Geospatial Foundation Model for Coarse-Resolution All-Sky Remote Sensing Imagery</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2411.17000">SatVision-TOA</a></td>
<td style="text-align: center;"><a href="https://github.com/nasa-nccs-hpda/pytorch-caney">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>RS-vHeat</strong></td>
<td><strong>RS-vHeat: Heat Conduction Guided Efficient Remote Sensing Foundation Model</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2411.17984">RS-vHeat</a></td>
<td style="text-align: center;">null</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Prithvi-EO-2.0</strong></td>
<td><strong>Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2412.02732">Prithvi-EO-2.0</a></td>
<td style="text-align: center;"><a href="https://github.com/NASA-IMPACT/Prithvi-EO-2.0">link</a></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>AnySat</strong></td>
<td><strong>AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities</strong></td>
<td style="text-align: center;">Arxiv2024</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2412.14123">AnySat</a></td>
<td style="text-align: center;"><a href="https://github.com/gastruc/AnySat">link</a></td>
</tr>
</tbody>
</table>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-astruc2024anysat" class="csl-entry" role="listitem">
Astruc, Guillaume, Nicolas Gonthier, Clement Mallet, and Loic Landrieu. 2024. <span>“<span>AnySat: An Earth</span> Observation Model for Any Resolutions, Scales, and Modalities.”</span> <em>arXiv Preprint arXiv:2412.14123</em>.
</div>
<div id="ref-fuller2023croma" class="csl-entry" role="listitem">
Fuller, Anthony, Koreen Millard, and James R Green. 2023. <span>“CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders.”</span> In <em>Thirty-Seventh Conference on Neural Information Processing Systems</em>.
</div>
<div id="ref-szwarcman_prithvi-eo-20_2024" class="csl-entry" role="listitem">
Szwarcman, Daniela, Sujit Roy, Paolo Fraccaro, orsteinn Elí Gíslason, Benedikt Blumenstiel, Rinki Ghosal, Pedro Henrique de Oliveira, et al. 2024. <span>“Prithvi-<span>EO-2</span>.0: <span>A Versatile Multi-Temporal Foundation Model</span> for <span>Earth Observation Applications</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2412.02732">https://doi.org/10.48550/arXiv.2412.02732</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./intro.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./propuesta.html" class="pagination-link" aria-label="Propuesta">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Propuesta</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Denis Berroeta</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>