@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}
@misc{19-eva,
	title = {19 - Evaluating Causal Models {\textemdash} Causal Inference for the Brave and True},
	url = {https://matheusfacure.github.io/python-causality-handbook/19-Evaluating-Causal-Models.html}
}

@article{xiao,
	title = {Foundation Models for Remote Sensing and Earth Observation: A Survey},
	author = {Xiao, Aoran and Xuan, Weihao and Wang, Junjue and Huang, Jiaxing and Tao, Dacheng and Lu, Shijian and Yokoya, Naoto},
	doi = {10.48550/arXiv.2410.16602},
	year={2024},
	langid = {en}
}

@article{jakubik,
	title = {Foundation Models for Generalist Geospatial Artificial Intelligence},
	author = {Jakubik, Johannes and Roy, Sujit and Phillips, C. E. and Fraccaro, Paolo and Godwin, Denys and Zadrozny, Bianca and Szwarcman, Daniela and Gomes, Carlos and Nyirjesy, Gabby and Edwards, Blair and Kimura, Daiki and Simumba, Naomi and Chu, Linsong and Mukkavilli, S. Karthik and Lambhate, Devyani and Das, Kamal and Bangalore, Ranjini and Oliveira, Dario and Muszynski, Michal and Ankur, Kumar and Ramasubramanian, Muthukumaran and Gurung, Iksha and Khallaghi, Sam and Hanxi,  and Li,  and Cecil, Michael and Ahmadi, Maryam and Kordi, Fatemeh and Alemohammad, Hamed and Maskey, Manil and Ganti, Raghu and Weldemariam, Kommy and Ramachandran, Rahul},
	doi = {10.48550/arXiv.2310.18660},
	year={2023},
	langid = {en}
}

@article{lu,
	title = {AI Foundation Models in Remote Sensing: A Survey},
	author = {Lu, Siqi and Guo, Junlin and Zimmer-Dauphinee, James R. and Nieusma, Jordan M. and Wang, Xiao and VanValkenburgh, Parker and Wernke, Steven A. and Huo, Yuankai},
	doi = {10.48550/arXiv.2408.03464},
	year={2024},
	langid = {en}
}

@inproceedings{fuller2023croma,
  title={CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders},
  author={Fuller, Anthony and Millard, Koreen and Green, James R},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@misc{szwarcman_prithvi-eo-20_2024,
  title = {Prithvi-{{EO-2}}.0: {{A Versatile Multi-Temporal Foundation Model}} for {{Earth Observation Applications}}},
  shorttitle = {Prithvi-{{EO-2}}.0},
  author = {Szwarcman, Daniela and Roy, Sujit and Fraccaro, Paolo and G{\'i}slason, {\TH}orsteinn El{\'i} and Blumenstiel, Benedikt and Ghosal, Rinki and de Oliveira, Pedro Henrique and Almeida, Joao Lucas de Sousa and Sedona, Rocco and Kang, Yanghui and Chakraborty, Srija and Wang, Sizhe and Kumar, Ankur and Truong, Myscon and Godwin, Denys and Lee, Hyunho and Hsu, Chia-Yu and Asanjan, Ata Akbari and Mujeci, Besart and Keenan, Trevor and Arevalo, Paulo and Li, Wenwen and Alemohammad, Hamed and Olofsson, Pontus and Hain, Christopher and Kennedy, Robert and Zadrozny, Bianca and Cavallaro, Gabriele and Watson, Campbell and Maskey, Manil and Ramachandran, Rahul and Moreno, Juan Bernabe},
  year = {2024},
  month = dec,
  number = {arXiv:2412.02732},
  eprint = {2412.02732},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.02732},
  urldate = {2025-01-07},
  abstract = {This technical report presents Prithvi-EO-2.0, a new geospatial foundation model that offers significant improvements over its predecessor, Prithvi-EO-1.0. Trained on 4.2M global time series samples from NASA's Harmonized Landsat and Sentinel-2 data archive at 30m resolution, the new 300M and 600M parameter models incorporate temporal and location embeddings for enhanced performance across various geospatial tasks. Through extensive benchmarking with GEOBench, the 600M version outperforms the previous Prithvi-EO model by 8\% across a range of tasks. It also outperforms six other geospatial foundation models when benchmarked on remote sensing tasks from different domains and resolutions (i.e. from 0.1m to 15m). The results demonstrate the versatility of the model in both classical earth observation and high-resolution applications. Early involvement of end-users and subject matter experts (SMEs) are among the key factors that contributed to the project's success. In particular, SME involvement allowed for constant feedback on model and dataset design, as well as successful customization for diverse SME-led applications in disaster response, land use and crop mapping, and ecosystem dynamics monitoring. Prithvi-EO-2.0 is available on Hugging Face and IBM terratorch, with additional resources on GitHub. The project exemplifies the Trusted Open Science approach embraced by all involved organizations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/denisberroeta/Zotero/storage/JP2HPLSC/Szwarcman et al. - 2024 - Prithvi-EO-2.0 A Versatile Multi-Temporal Foundat.pdf}
}

@article{astruc2024anysat,
  title={{AnySat: An Earth} Observation Model for Any Resolutions, Scales, and Modalities},
  author={Astruc, Guillaume and Gonthier, Nicolas and Mallet, Clement and Landrieu, Loic},
  journal={arXiv preprint arXiv:2412.14123},
  year={2024}
}

