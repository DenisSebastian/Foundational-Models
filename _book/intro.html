<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.5">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Introduction – Modelos Fundacionales de IA</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./papers_review_vfm.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ff4371ef257df69894857e99c6ad0d06.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles/style.css">
</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./intro.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Modelos Fundacionales de IA</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./papers_review_vfm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Papers Review: VFM</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./papers_review_vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Papers Review: VLM</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./propuesta.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Propuesta</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#conceptos-de-modelos-fundacionales" id="toc-conceptos-de-modelos-fundacionales" class="nav-link active" data-scroll-target="#conceptos-de-modelos-fundacionales"><span class="header-section-number">1.1</span> Conceptos de Modelos Fundacionales</a>
  <ul class="collapse">
  <li><a href="#modelo-fundacional-mf" id="toc-modelo-fundacional-mf" class="nav-link" data-scroll-target="#modelo-fundacional-mf"><span class="header-section-number">1.1.1</span> Modelo fundacional (MF)</a></li>
  <li><a href="#pre-entrenamiento" id="toc-pre-entrenamiento" class="nav-link" data-scroll-target="#pre-entrenamiento"><span class="header-section-number">1.1.2</span> Pre-entrenamiento</a></li>
  <li><a href="#fine-tuning" id="toc-fine-tuning" class="nav-link" data-scroll-target="#fine-tuning"><span class="header-section-number">1.1.3</span> Fine-Tuning</a></li>
  </ul></li>
  <li><a href="#tipos-de-modelos-fundacionales" id="toc-tipos-de-modelos-fundacionales" class="nav-link" data-scroll-target="#tipos-de-modelos-fundacionales"><span class="header-section-number">1.2</span> Tipos de Modelos Fundacionales</a></li>
  <li><a href="#arquitecturas-comunes" id="toc-arquitecturas-comunes" class="nav-link" data-scroll-target="#arquitecturas-comunes"><span class="header-section-number">1.3</span> Arquitecturas Comunes</a></li>
  <li><a href="#datasets-en-remote-sensing" id="toc-datasets-en-remote-sensing" class="nav-link" data-scroll-target="#datasets-en-remote-sensing"><span class="header-section-number">1.4</span> 5. Datasets en Remote Sensing</a></li>
  <li><a href="#metodologías-de-pre-entrenamiento" id="toc-metodologías-de-pre-entrenamiento" class="nav-link" data-scroll-target="#metodologías-de-pre-entrenamiento"><span class="header-section-number">1.5</span> Metodologías de Pre-Entrenamiento</a>
  <ul class="collapse">
  <li><a href="#supervised-learning" id="toc-supervised-learning" class="nav-link" data-scroll-target="#supervised-learning"><span class="header-section-number">1.5.1</span> Supervised Learning</a></li>
  <li><a href="#self-supervised-learning-ssl" id="toc-self-supervised-learning-ssl" class="nav-link" data-scroll-target="#self-supervised-learning-ssl"><span class="header-section-number">1.5.2</span> Self-Supervised Learning (SSL)</a></li>
  </ul></li>
  <li><a href="#aplicaciones-en-tareas-de-remote-sensing" id="toc-aplicaciones-en-tareas-de-remote-sensing" class="nav-link" data-scroll-target="#aplicaciones-en-tareas-de-remote-sensing"><span class="header-section-number">1.6</span> Aplicaciones en Tareas de Remote Sensing</a>
  <ul class="collapse">
  <li><a href="#clasificación-de-escenas" id="toc-clasificación-de-escenas" class="nav-link" data-scroll-target="#clasificación-de-escenas"><span class="header-section-number">1.6.1</span> Clasificación de Escenas</a></li>
  <li><a href="#segmentación-semántica" id="toc-segmentación-semántica" class="nav-link" data-scroll-target="#segmentación-semántica"><span class="header-section-number">1.6.2</span> Segmentación Semántica</a></li>
  <li><a href="#detección-de-objetos" id="toc-detección-de-objetos" class="nav-link" data-scroll-target="#detección-de-objetos"><span class="header-section-number">1.6.3</span> Detección de Objetos</a></li>
  <li><a href="#detección-de-cambios" id="toc-detección-de-cambios" class="nav-link" data-scroll-target="#detección-de-cambios"><span class="header-section-number">1.6.4</span> Detección de Cambios</a></li>
  <li><a href="#imputación-de-nubes" id="toc-imputación-de-nubes" class="nav-link" data-scroll-target="#imputación-de-nubes"><span class="header-section-number">1.6.5</span> Imputación de Nubes</a></li>
  </ul></li>
  <li><a href="#desafíos-y-oportunidades" id="toc-desafíos-y-oportunidades" class="nav-link" data-scroll-target="#desafíos-y-oportunidades"><span class="header-section-number">1.7</span> Desafíos y Oportunidades</a>
  <ul class="collapse">
  <li><a href="#desafíos" id="toc-desafíos" class="nav-link" data-scroll-target="#desafíos"><span class="header-section-number">1.7.1</span> Desafíos</a></li>
  <li><a href="#oportunidades" id="toc-oportunidades" class="nav-link" data-scroll-target="#oportunidades"><span class="header-section-number">1.7.2</span> Oportunidades</a></li>
  </ul></li>
  <li><a href="#referencias" id="toc-referencias" class="nav-link" data-scroll-target="#referencias"><span class="header-section-number">1.8</span> Referencias</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>El <strong>Remote Sensing</strong> juega un papel fundamental en la observación y análisis de la Tierra, permitiendo el monitoreo continuo y detallado de nuestro planeta a través de tecnologías satelitales y aéreas. Desde la detección de cambios ambientales hasta la gestión de desastres naturales, su relevancia se ha incrementado exponencialmente en los últimos años gracias a la creciente disponibilidad de datos de alta resolución y la evolución de los métodos de procesamiento.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/Foundation Models for Remote Sensing.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Idea General de Modelos Fundacionales en Remote Sensing</figcaption>
</figure>
</div>
<p>En este contexto, los <strong>modelos fundacionales (FMs)</strong> representan una innovación significativa. Estos modelos pre-entrenados, capaces de aprender representaciones generales a partir de grandes volúmenes de datos no etiquetados, permiten una adaptación eficiente a tareas específicas con la necesidad de pocos datos anotados. La versatilidad de los FMs radica en su capacidad de abordar múltiples aplicaciones en Remote Sensing, como la segmentación de imágenes, detección de objetos y clasificación de escenas, optimizando así el análisis de datos complejos.</p>
<section id="conceptos-de-modelos-fundacionales" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="conceptos-de-modelos-fundacionales"><span class="header-section-number">1.1</span> Conceptos de Modelos Fundacionales</h2>
<section id="modelo-fundacional-mf" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="modelo-fundacional-mf"><span class="header-section-number">1.1.1</span> Modelo fundacional (MF)</h3>
<p>Un <strong>modelo fundacional</strong> se define como un modelo de inteligencia artificial pre-entrenado en grandes volúmenes de datos no etiquetados que puede ser ajustado (fine-tuned) a una variedad de tareas específicas con un esfuerzo computacional significativamente menor. Estos modelos han demostrado capacidades avanzadas de generalización, adaptabilidad y eficiencia en dominios complejos como el <em>Remote Sensing</em> <span class="citation" data-cites="xiao">(<a href="references.html#ref-xiao" role="doc-biblioref">Xiao et al. 2024</a>)</span>.</p>
<p>En el ámbito de <strong>Remote Sensing</strong>, los modelos fundacionales aprovechan datos multiespectrales, temporales y de alta resolución espacial obtenidos de sensores satelitales y aéreos. Su aplicación permite resolver tareas críticas como la clasificación de escenas, segmentación semántica, detección de objetos y detección de cambios con alta precisión y eficiencia computacional <span class="citation" data-cites="jakubik">(<a href="references.html#ref-jakubik" role="doc-biblioref">Jakubik et al. 2023</a>)</span>.</p>
</section>
<section id="pre-entrenamiento" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="pre-entrenamiento"><span class="header-section-number">1.1.2</span> Pre-entrenamiento</h3>
<p>El <strong>pre-entrenamiento</strong> es el proceso mediante el cual un modelo aprende representaciones generales del dominio a partir de grandes volúmenes de datos no etiquetados. Esta etapa utiliza técnicas de aprendizaje auto-supervisado (SSL) como el <em>contrastive learning</em> y el <em>masked autoencoding</em>, permitiendo al modelo captar patrones, relaciones y estructuras en los datos sin intervención manual en el etiquetado (Lu et al., 2024). En el caso del Remote Sensing, el pre-entrenamiento se lleva a cabo sobre datos satelitales masivos como Sentinel-2, BigEarthNet o HLS <span class="citation" data-cites="jakubik">(<a href="references.html#ref-jakubik" role="doc-biblioref">Jakubik et al. 2023</a>)</span>.</p>
</section>
<section id="fine-tuning" class="level3" data-number="1.1.3">
<h3 data-number="1.1.3" class="anchored" data-anchor-id="fine-tuning"><span class="header-section-number">1.1.3</span> Fine-Tuning</h3>
<p>El <strong>fine-tuning</strong> consiste en ajustar un modelo fundacional pre-entrenado a una tarea específica utilizando un conjunto de datos etiquetados más reducido. Durante esta etapa, el modelo reutiliza las representaciones generales aprendidas en el pre-entrenamiento y las optimiza para una aplicación concreta, como la segmentación de imágenes o la detección de objetos. Este enfoque mejora la eficiencia del modelo al reducir la necesidad de grandes cantidades de datos etiquetados y recursos computacionales <span class="citation" data-cites="xiao">(<a href="references.html#ref-xiao" role="doc-biblioref">Xiao et al. 2024</a>)</span>.</p>
<p>En resumen, los modelos fundacionales revolucionan el <em>Remote Sensing</em> al permitir la aplicación eficiente de técnicas avanzadas de inteligencia artificial en tareas complejas y multiespectrales, facilitando el análisis de datos geoespaciales a escala global.</p>
</section>
</section>
<section id="tipos-de-modelos-fundacionales" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="tipos-de-modelos-fundacionales"><span class="header-section-number">1.2</span> Tipos de Modelos Fundacionales</h2>
<p><strong>Visual Foundation Models (VFMs)</strong>: Estos modelos se centran en el procesamiento y análisis de datos visuales, abordando tareas como segmentación semántica, detección de objetos y clasificación de escenas. Utilizan arquitecturas avanzadas como Vision Transformers (ViT) y enfoques auto-supervisados para maximizar la eficiencia en escenarios con datos multiespectrales y de alta resolución <span class="citation" data-cites="jakubik">(<a href="references.html#ref-jakubik" role="doc-biblioref">Jakubik et al. 2023</a>)</span>, <span class="citation" data-cites="lu">(<a href="references.html#ref-lu" role="doc-biblioref">Lu et al. 2024</a>)</span>.</p>
<p><img src="images/SSL.png" class="img-fluid"></p>
<p><strong>Vision-Language Models (VLMs)</strong>: Diseñados para integrar datos visuales y textuales, estos modelos combinan imágenes satelitales con descripciones textuales para realizar tareas como visual grounding, generación de descripciones automáticas y respuestas a preguntas basadas en imágenes. Su capacidad multimodal permite aplicaciones más intuitivas y precisas en el campo del Remote Sensing <span class="citation" data-cites="xiao">(<a href="references.html#ref-xiao" role="doc-biblioref">Xiao et al. 2024</a>)</span> .</p>
<p><img src="images/multitask_VLM.png" class="img-fluid"></p>
</section>
<section id="arquitecturas-comunes" class="level2 page-columns page-full" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="arquitecturas-comunes"><span class="header-section-number">1.3</span> Arquitecturas Comunes</h2>
<p><strong>Convolutional Neural Networks (CNNs):</strong> Estas redes son ampliamente utilizadas por su capacidad para extraer características jerárquicas de imágenes a través de operaciones convolucionales. En Remote Sensing, han demostrado eficacia en tareas como clasificación de imágenes y detección de objetos, pero enfrentan limitaciones en la captura de dependencias globales en imágenes de alta resolución <span class="citation" data-cites="lu">(<a href="references.html#ref-lu" role="doc-biblioref">Lu et al. 2024</a>)</span>.</p>
<p><strong>Vision Transformers (ViT):</strong> Los Transformers son arquitecturas basadas en mecanismos de <em>self-attention</em> que permiten modelar dependencias de largo alcance en imágenes dividiéndolas en parches tratados como secuencias. Su flexibilidad y precisión los hacen ideales para tareas como la segmentación semántica en datos multiespectrales y multitemporales (Jakubik et al., 2023).</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/ViT.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">ViT.png</figcaption>
</figure>
</div>
<p><strong>Masked Autoencoder (MAE):</strong> Este modelo utiliza el enfoque de reconstrucción de imágenes donde una fracción significativa de los píxeles se enmascara durante el entrenamiento, y el modelo aprende a predecir las partes faltantes. En Remote Sensing, los MAE son efectivos para captar patrones en datos satelitales multiespectrales y temporales, optimizando la eficiencia en tareas de imputación y análisis de imágenes incompletas <span class="citation" data-cites="xiao">(<a href="references.html#ref-xiao" role="doc-biblioref">Xiao et al. 2024</a>)</span>.</p>
</section>
<section id="datasets-en-remote-sensing" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="datasets-en-remote-sensing"><span class="header-section-number">1.4</span> 5. Datasets en Remote Sensing</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Nombre del Dataset</th>
<th>Tamaño</th>
<th>Resolución</th>
<th>Características Clave</th>
<th>Paper de Referencia</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sentinel-1/2</td>
<td>1M+ imágenes</td>
<td>10-30m</td>
<td>Multiespectral y temporal</td>
<td><span class="citation" data-cites="xiao">(<a href="references.html#ref-xiao" role="doc-biblioref">Xiao et al. 2024</a>)</span></td>
</tr>
<tr class="even">
<td>Harmonized Landsat-Sentinel (HLS)</td>
<td>1TB</td>
<td>30m</td>
<td>Imagen multitemporal y multiespectral</td>
<td><span class="citation" data-cites="jakubik">(<a href="references.html#ref-jakubik" role="doc-biblioref">Jakubik et al. 2023</a>)</span></td>
</tr>
<tr class="odd">
<td>BigEarthNet</td>
<td>590K+</td>
<td>10-60m</td>
<td>Cobertura en Europa</td>
<td><span class="citation" data-cites="lu">(<a href="references.html#ref-lu" role="doc-biblioref">Lu et al. 2024</a>)</span></td>
</tr>
<tr class="even">
<td>SSL4EO-S12</td>
<td>3M+</td>
<td>10-60m</td>
<td>Multimodal (SAR + óptico)</td>
<td><span class="citation" data-cites="xiao">(<a href="references.html#ref-xiao" role="doc-biblioref">Xiao et al. 2024</a>)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="metodologías-de-pre-entrenamiento" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="metodologías-de-pre-entrenamiento"><span class="header-section-number">1.5</span> Metodologías de Pre-Entrenamiento</h2>
<section id="supervised-learning" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="supervised-learning"><span class="header-section-number">1.5.1</span> Supervised Learning</h3>
<p>El pre-entrenamiento con datos etiquetados, como en el caso del dataset MillionAID, consiste en entrenar modelos utilizando grandes volúmenes de datos previamente anotados con etiquetas precisas. Este enfoque facilita la capacidad del modelo para capturar representaciones complejas y específicas del dominio, lo que resulta particularmente efectivo en tareas como la clasificación de escenas y la detección de objetos en Remote Sensing <span class="citation" data-cites="lu">(<a href="references.html#ref-lu" role="doc-biblioref">Lu et al. 2024</a>)</span>. En este contexto, MillionAID se ha convertido en un recurso clave al proporcionar un conjunto diverso de imágenes que abordan múltiples categorías globales, contribuyendo significativamente al desarrollo de modelos robustos y generalizables <span class="citation" data-cites="xiao">(<a href="references.html#ref-xiao" role="doc-biblioref">Xiao et al. 2024</a>)</span>.</p>
</section>
<section id="self-supervised-learning-ssl" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="self-supervised-learning-ssl"><span class="header-section-number">1.5.2</span> Self-Supervised Learning (SSL)</h3>
<p><strong>Contrastive Learning</strong>: Este enfoque utiliza pares positivos y negativos para maximizar la similitud entre representaciones de datos similares y minimizarla entre diferentes. Los pares positivos se generan generalmente aplicando transformaciones a la misma muestra, como recortes, rotaciones o cambios de escala, mientras que los pares negativos provienen de diferentes muestras dentro del lote de datos. Este mecanismo fuerza al modelo a aprender representaciones invariantes a estas transformaciones, lo que es crucial en datos multitemporales. Por ejemplo, SeCo emplea aprendizaje contrastivo para capturar invariancias temporales en datos de observación terrestre, mientras que GASSL integra geolocalización y aprendizaje temporal para enriquecer las representaciones, aprovechando datos satelitales masivos <span class="citation" data-cites="xiao">(<a href="references.html#ref-xiao" role="doc-biblioref">Xiao et al. 2024</a>)</span>.</p>
<p><strong>Masked Autoencoding</strong>: En este método, el modelo aprende a reconstruir las partes faltantes de una entrada, lo que mejora su comprensión de estructuras globales. SatMAE es un ejemplo clave que se enfoca en datos multiespectrales y multitemporales, mientras que Scale-MAE incorpora información de diferentes escalas para optimizar el aprendizaje <span class="citation" data-cites="jakubik">(<a href="references.html#ref-jakubik" role="doc-biblioref">Jakubik et al. 2023</a>)</span>.</p>
<p><strong>Multimodal Pre-training</strong>: Este enfoque integra diferentes tipos de datos, como SAR, ópticos y auxiliares, para mejorar la capacidad del modelo en tareas complejas. La combinación de modalidades permite al modelo captar patrones complementarios y abordar la heterogeneidad en datos de Remote Sensing <span class="citation" data-cites="lu">(<a href="references.html#ref-lu" role="doc-biblioref">Lu et al. 2024</a>)</span>.</p>
</section>
</section>
<section id="aplicaciones-en-tareas-de-remote-sensing" class="level2 page-columns page-full" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="aplicaciones-en-tareas-de-remote-sensing"><span class="header-section-number">1.6</span> Aplicaciones en Tareas de Remote Sensing</h2>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/FM_tasks.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">FM_tasks.png</figcaption>
</figure>
</div>
<section id="clasificación-de-escenas" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="clasificación-de-escenas"><span class="header-section-number">1.6.1</span> Clasificación de Escenas</h3>
<p>La clasificación de escenas es el proceso mediante el cual se categorizan imágenes satelitales completas en clases predefinidas, como áreas urbanas, forestales, cuerpos de agua o agrícolas. Este enfoque permite extraer información útil para tareas como la gestión de recursos naturales, el monitoreo ambiental y la planificación urbana. Modelos como <strong>GeoKR</strong> y <strong>CSPT</strong> son ejemplos destacados que emplean arquitecturas avanzadas para mejorar la precisión y escalabilidad en este tipo de tareas.</p>
</section>
<section id="segmentación-semántica" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" class="anchored" data-anchor-id="segmentación-semántica"><span class="header-section-number">1.6.2</span> Segmentación Semántica</h3>
<p>La segmentación semántica es una tarea crítica en Remote Sensing que asigna una etiqueta a cada píxel de una imagen, permitiendo la identificación precisa de diferentes tipos de cobertura terrestre como agua, bosques y áreas urbanas. Este nivel de detalle es esencial para aplicaciones como la gestión ambiental, la planificación urbana y la agricultura de precisión. Por ejemplo, <strong>Prithvi</strong> ha demostrado ser efectivo en tareas de segmentación multitemporal, aprovechando datos multiespectrales para mapear dinámicamente cambios en el uso de la tierra. De manera similar, <strong>SatMAE++</strong> utiliza enfoques avanzados de auto-codificación para mejorar la segmentación en imágenes con variabilidad de escalas, logrando una alta precisión incluso en escenarios complejos <span class="citation" data-cites="jakubik">(<a href="references.html#ref-jakubik" role="doc-biblioref">Jakubik et al. 2023</a>)</span>, <span class="citation" data-cites="xiao">(<a href="references.html#ref-xiao" role="doc-biblioref">Xiao et al. 2024</a>)</span>.</p>
</section>
<section id="detección-de-objetos" class="level3" data-number="1.6.3">
<h3 data-number="1.6.3" class="anchored" data-anchor-id="detección-de-objetos"><span class="header-section-number">1.6.3</span> Detección de Objetos</h3>
<p>La detección de objetos es un proceso en el cual se identifican y localizan entidades específicas dentro de imágenes satelitales, como edificios, vehículos o embarcaciones. Este enfoque es fundamental para aplicaciones como el monitoreo urbano, la gestión de desastres y la supervisión de infraestructuras críticas. Por ejemplo, <strong>RingMo</strong> utiliza enfoques avanzados de aprendizaje multimodal para mejorar la detección en imágenes multiespectrales y multitemporales. Por otro lado, <strong>SkySense</strong> combina datos ópticos y SAR para abordar desafíos relacionados con variaciones ambientales y características espectrales complejas, proporcionando una mayor robustez y precisión en tareas de detección <span class="citation" data-cites="xiao">(<a href="references.html#ref-xiao" role="doc-biblioref">Xiao et al. 2024</a>)</span>, <span class="citation" data-cites="lu">(<a href="references.html#ref-lu" role="doc-biblioref">Lu et al. 2024</a>)</span></p>
</section>
<section id="detección-de-cambios" class="level3" data-number="1.6.4">
<h3 data-number="1.6.4" class="anchored" data-anchor-id="detección-de-cambios"><span class="header-section-number">1.6.4</span> Detección de Cambios</h3>
<p>La detección de cambios implica el uso de imágenes multitemporales para identificar y analizar alteraciones en la superficie terrestre a lo largo del tiempo. Este proceso es fundamental para aplicaciones como el monitoreo de deforestación, expansiones urbanas y desastres naturales. Modelos como <strong>GeoChange</strong> y <strong>ChangeSTAR</strong> se utilizan para procesar grandes volúmenes de datos satelitales, integrando algoritmos que permiten una comparación precisa entre diferentes momentos temporales, destacando incluso los cambios más sutiles (<span class="citation" data-cites="jakubik">(<a href="references.html#ref-jakubik" role="doc-biblioref">Jakubik et al. 2023</a>)</span>, <span class="citation" data-cites="xiao">(<a href="references.html#ref-xiao" role="doc-biblioref">Xiao et al. 2024</a>)</span>).</p>
</section>
<section id="imputación-de-nubes" class="level3" data-number="1.6.5">
<h3 data-number="1.6.5" class="anchored" data-anchor-id="imputación-de-nubes"><span class="header-section-number">1.6.5</span> Imputación de Nubes</h3>
<p>El proceso de imputación de nubes, conocido como <strong>cloud gap filling</strong>, se refiere a la tarea de reconstruir áreas de datos faltantes en imágenes satelitales debido a la presencia de nubes. Este problema es crítico para garantizar la continuidad espacial y temporal de los datos en aplicaciones como el monitoreo de cultivos o el análisis ambiental. Por ejemplo, <strong>Prithvi</strong> emplea algoritmos basados en modelos fundacionales para rellenar estos vacíos de manera precisa, utilizando datos multiespectrales y multitemporales como referencia contextual, lo que mejora significativamente la calidad de las observaciones satelitales <span class="citation" data-cites="xiao">(<a href="references.html#ref-xiao" role="doc-biblioref">Xiao et al. 2024</a>)</span>.</p>
</section>
</section>
<section id="desafíos-y-oportunidades" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="desafíos-y-oportunidades"><span class="header-section-number">1.7</span> Desafíos y Oportunidades</h2>
<section id="desafíos" class="level3" data-number="1.7.1">
<h3 data-number="1.7.1" class="anchored" data-anchor-id="desafíos"><span class="header-section-number">1.7.1</span> Desafíos</h3>
<ul>
<li><strong>Escasez de datos etiquetados</strong> para pre-entrenamiento.</li>
<li><strong>Costos computacionales</strong>: entrenamiento en grandes volúmenes de datos.</li>
<li><strong>Generalización limitada</strong> en dominios multiespectrales y multimodales.</li>
</ul>
</section>
<section id="oportunidades" class="level3" data-number="1.7.2">
<h3 data-number="1.7.2" class="anchored" data-anchor-id="oportunidades"><span class="header-section-number">1.7.2</span> Oportunidades</h3>
<ul>
<li>Integración de <strong>modelos multimodales</strong> (e.g., VLMs para texto-imagen).</li>
<li>Avances en <strong>transferencia de conocimiento</strong> entre dominios geoespaciales.</li>
</ul>
</section>
</section>
<section id="referencias" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="referencias"><span class="header-section-number">1.8</span> Referencias</h2>
<p>Jakubik et al., <em>“Foundation Models for Generalist Geospatial AI”</em>, 2023.</p>
<p>Xiao et al., <em>“Foundation Models for Remote Sensing and Earth Observation: A Survey”</em>, 2024.</p>
<p>Lu et al., <em>“AI Foundation Models in Remote Sensing: A Survey”</em>, 2024. Otros artículos relevantes. -</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-jakubik" class="csl-entry" role="listitem">
Jakubik, Johannes, Sujit Roy, C. E. Phillips, Paolo Fraccaro, Denys Godwin, Bianca Zadrozny, Daniela Szwarcman, et al. 2023. <span>“Foundation Models for Generalist Geospatial Artificial Intelligence.”</span> <a href="https://doi.org/10.48550/arXiv.2310.18660">https://doi.org/10.48550/arXiv.2310.18660</a>.
</div>
<div id="ref-lu" class="csl-entry" role="listitem">
Lu, Siqi, Junlin Guo, James R. Zimmer-Dauphinee, Jordan M. Nieusma, Xiao Wang, Parker VanValkenburgh, Steven A. Wernke, and Yuankai Huo. 2024. <span>“AI Foundation Models in Remote Sensing: A Survey.”</span> <a href="https://doi.org/10.48550/arXiv.2408.03464">https://doi.org/10.48550/arXiv.2408.03464</a>.
</div>
<div id="ref-xiao" class="csl-entry" role="listitem">
Xiao, Aoran, Weihao Xuan, Junjue Wang, Jiaxing Huang, Dacheng Tao, Shijian Lu, and Naoto Yokoya. 2024. <span>“Foundation Models for Remote Sensing and Earth Observation: A Survey.”</span> <a href="https://doi.org/10.48550/arXiv.2410.16602">https://doi.org/10.48550/arXiv.2410.16602</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Preface">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./papers_review_vfm.html" class="pagination-link" aria-label="Papers Review: VFM">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Papers Review: VFM</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Denis Berroeta</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>